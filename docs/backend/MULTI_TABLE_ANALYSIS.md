# å¤šè¡¨å…³è”åˆ†æåŠŸèƒ½ - å®Œæ•´å®ç°æ–‡æ¡£

## ä¸€ã€åŠŸèƒ½æ¦‚è¿°

**ç‰ˆæœ¬**ï¼šv1.0  
**å®Œæˆæ—¥æœŸ**ï¼š2026-01-09  
**æ ¸å¿ƒèƒ½åŠ›**ï¼šå°†ç³»ç»Ÿä»å•è¡¨åˆ†æå‡çº§ä¸ºå¤šè¡¨å…³è”åˆ†æï¼Œæ”¯æŒæ‰¹é‡ä¸Šä¼ å¤šä¸ª Excel/CSV æ–‡ä»¶ï¼Œé€šè¿‡ AI æ™ºèƒ½è¯†åˆ«è¡¨å…³ç³»ï¼Œå®ç°è·¨è¡¨ JOIN æŸ¥è¯¢ã€‚

### æŠ€æœ¯äº®ç‚¹

âœ… **DuckDB é›†æˆ**ï¼šåµŒå…¥å¼ OLAP å¼•æ“ï¼Œé›¶é…ç½®ï¼Œé«˜æ€§èƒ½åˆ—å¼å­˜å‚¨  
âœ… **LLM å¢å¼ºå…³ç³»æ¨ç†**ï¼šç»“åˆè§„åˆ™å¼•æ“ã€æ•°æ®é‡‡æ ·å’Œ Qwen-Maxï¼Œå‡†ç¡®è¯†åˆ«å¤–é”®å…³ç³»  
âœ… **å¯è§†åŒ–å»ºæ¨¡**ï¼šVue Flow ER å›¾å±•ç¤ºï¼Œæ”¯æŒäººæœºååŒç¡®è®¤å…³ç³»  
âœ… **æ— ç¼å…¼å®¹**ï¼šä¿ç•™åŸæœ‰å•è¡¨åˆ†æèƒ½åŠ›ï¼Œå‘åå…¼å®¹ä¼ ç»Ÿæ•°æ®æº

---

## äºŒã€æ¶æ„è®¾è®¡

### 2.1 æ•°æ®æµæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç”¨æˆ·ä¸Šä¼     â”‚
â”‚ 3ä¸ªCSVæ–‡ä»¶  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åç«¯ API             â”‚
â”‚ /upload/multi-files  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â–¶ è§£ææ–‡ä»¶ (Pandas)
       â”œâ”€â–¶ åˆ›å»º DuckDB æ•°æ®åº“
       â”œâ”€â–¶ å¯¼å…¥æ‰€æœ‰è¡¨
       â”œâ”€â–¶ åˆ›å»º Dataset è®°å½•
       â””â”€â–¶ åå°è®­ç»ƒ Vanna DDL
       
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI å…³ç³»æ¨ç†           â”‚
â”‚ /dataset/analyze      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â–¶ è§„åˆ™åˆç­›ï¼ˆå‘½åæ¨¡å¼åŒ¹é…ï¼‰
        â”œâ”€â–¶ LLM æ·±åº¦åˆ†æï¼ˆQwen-Maxï¼‰
        â””â”€â–¶ æ•°æ®é‡åˆåº¦éªŒè¯ï¼ˆSQL INTERSECTï¼‰
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¯è§†åŒ–å»ºæ¨¡             â”‚
â”‚ VueFlow ER å›¾          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–¶ è‡ªåŠ¨å¸ƒå±€ï¼ˆDagreï¼‰
         â”œâ”€â–¶ æ¸²æŸ“ AI æ¨èå…³ç³»ï¼ˆè™šçº¿ï¼‰
         â””â”€â–¶ ç”¨æˆ·ç¡®è®¤/ä¿®æ­£
         
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å…³ç³»è®­ç»ƒ               â”‚
â”‚ /dataset/.../config    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â–¶ è®­ç»ƒå…³ç³»æè¿°åˆ° Vanna
         
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è‡ªç„¶è¯­è¨€æŸ¥è¯¢           â”‚
â”‚ /chat                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–¶ Vanna ç”Ÿæˆ JOIN SQL
         â”œâ”€â–¶ DuckDB æ‰§è¡ŒæŸ¥è¯¢
         â””â”€â–¶ è¿”å›ç»“æœ + å›¾è¡¨
```

### 2.2 æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | æ–‡ä»¶è·¯å¾„ | åŠŸèƒ½æè¿° |
|------|---------|---------|
| **DuckDB æœåŠ¡** | `backend/app/services/duckdb_service.py` | æ•°æ®åº“ç®¡ç†ã€æ•°æ®å¯¼å…¥ã€æŸ¥è¯¢æ‰§è¡Œ |
| **å…³ç³»åˆ†æå™¨** | `backend/app/services/vanna/relationship_analyzer.py` | LLM å¢å¼ºçš„è¡¨å…³ç³»æ¨ç† |
| **å¤šæ–‡ä»¶ä¸Šä¼  API** | `backend/app/api/v1/endpoints/upload.py` | æ‰¹é‡æ–‡ä»¶ä¸Šä¼ å¤„ç† |
| **è®­ç»ƒæœåŠ¡å¢å¼º** | `backend/app/services/vanna/training_service.py` | æ”¯æŒ DuckDB DDL è®­ç»ƒ |
| **SQL ç”Ÿæˆå™¨** | `backend/app/services/vanna/sql_generator.py` | å…¼å®¹ DuckDB çš„æŸ¥è¯¢æ‰§è¡Œ |
| **æ‰¹é‡ä¸Šä¼ ç»„ä»¶** | `frontend/src/views/Dataset/MultiFileUpload.vue` | å‰ç«¯ä¸Šä¼ ç•Œé¢ |

---

## ä¸‰ã€åç«¯æ ¸å¿ƒå®ç°

### 3.1 DuckDB æœåŠ¡ (`duckdb_service.py`)

**è®¾è®¡ç†å¿µ**ï¼šä¸ºæ¯ä¸ª Dataset åˆ›å»ºç‹¬ç«‹çš„ DuckDB æ•°æ®åº“æ–‡ä»¶ï¼Œç¡®ä¿æ•°æ®éš”ç¦»å’Œç®¡ç†ç®€ä¾¿ã€‚

#### å…³é”®æ–¹æ³•

```python
class DuckDBService:
    """DuckDB æ•°æ®åº“ç®¡ç†æœåŠ¡"""
    
    @classmethod
    def create_dataset_database(cls, dataset_id: int) -> str:
        """ä¸ºæ•°æ®é›†åˆ›å»º DuckDB æ•°æ®åº“
        
        è¿”å›è·¯å¾„ï¼šduckdb_storage/dataset_{id}.db
        """
    
    @classmethod
    def import_dataframes(cls, db_path: str, dataframes: Dict[str, pd.DataFrame]) -> Dict[str, int]:
        """æ‰¹é‡å¯¼å…¥å¤šä¸ª DataFrame
        
        è‡ªåŠ¨æ¸…æ´—åˆ—åï¼š
        - æ›¿æ¢ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        - ä½¿ç”¨ CREATE OR REPLACE TABLE ç¡®ä¿å¹‚ç­‰æ€§
        """
    
    @classmethod
    def execute_query(cls, db_path: str, sql: str) -> pd.DataFrame:
        """æ‰§è¡Œ SQL æŸ¥è¯¢
        
        read_only æ¨¡å¼ç¡®ä¿æŸ¥è¯¢ä¸ä¿®æ”¹æ•°æ®
        """
    
    @classmethod
    def get_table_schema(cls, db_path: str, table_name: str) -> List[Dict]:
        """è·å–è¡¨ç»“æ„ï¼ˆç”¨äº ER å›¾æ¸²æŸ“å’Œè®­ç»ƒï¼‰"""
    
    @classmethod
    def get_table_ddl(cls, db_path: str, table_name: str) -> str:
        """ç”Ÿæˆ DDL è¯­å¥ï¼ˆç”¨äº Vanna è®­ç»ƒï¼‰"""
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
# 1. åˆ›å»ºæ•°æ®åº“
db_path = DuckDBService.create_dataset_database(dataset_id=123)

# 2. å¯¼å…¥æ•°æ®
dataframes = {
    'orders': orders_df,
    'customers': customers_df,
    'products': products_df
}
stats = DuckDBService.import_dataframes(db_path, dataframes)
# stats: {'orders': 5230, 'customers': 1842, 'products': 456}

# 3. æŸ¥è¯¢
sql = "SELECT c.name, COUNT(*) as order_count FROM orders o LEFT JOIN customers c ON o.customer_id = c.id GROUP BY c.name"
result_df = DuckDBService.execute_query(db_path, sql)
```

---

### 3.2 å…³ç³»åˆ†æå™¨ (`relationship_analyzer.py`)

**è®¾è®¡ç†å¿µ**ï¼šä¸‰å±‚åˆ†ææ¶æ„ = è§„åˆ™å¿«ç­› + LLM æ·±åº¦ç†è§£ + æ•°æ®éªŒè¯

#### åˆ†ææµç¨‹

```python
class RelationshipAnalyzer:
    @classmethod
    def analyze_relationships(cls, dataset_id, db_path, table_names):
        """å®Œæ•´åˆ†ææµç¨‹"""
        
        # Layer 1: è§„åˆ™åˆç­›ï¼ˆç§’çº§ï¼‰
        candidates = cls._rule_based_filtering(schemas)
        # è§„åˆ™ï¼š
        # - orders.customer_id vs customers.id
        # - æ•°æ®ç±»å‹å…¼å®¹æ€§æ£€æŸ¥
        
        # Layer 2: LLM æ·±åº¦åˆ†æï¼ˆ10-30ç§’ï¼‰
        relationships = cls._llm_analysis(schemas, candidates)
        # Prompt å·¥ç¨‹ï¼š
        # - æä¾› Schema + æ ·æœ¬æ•°æ®
        # - è¦æ±‚è¿”å› JSON æ•°ç»„
        # - åŒ…å« confidence å’Œ reasoning
        
        # Layer 3: æ•°æ®é‡åˆåº¦éªŒè¯ï¼ˆç§’çº§ï¼‰
        for rel in relationships:
            overlap = cls._calculate_data_overlap(db_path, rel)
            # SQL: SELECT COUNT(DISTINCT val) FROM (A INTERSECT B)
            rel['data_overlap'] = overlap  # 98.5%
        
        return relationships
```

#### LLM Prompt è®¾è®¡

```python
prompt = f"""åˆ†æä»¥ä¸‹æ•°æ®è¡¨ç»“æ„ï¼Œè¯†åˆ«æ½œåœ¨çš„å¤–é”®å…³ç³»ï¼š

**è¡¨ç»“æ„ä¿¡æ¯ï¼š**
{json.dumps(simplified_schemas, ensure_ascii=False, indent=2)}

**åˆ†æç»´åº¦ï¼š**
1. **å‘½åçº¦å®š**ï¼šä¾‹å¦‚ orders.customer_id åº”å…³è”åˆ° customers.id
2. **æ•°æ®ç±»å‹**ï¼šç¡®ä¿å­—æ®µç±»å‹å…¼å®¹ï¼ˆINT-INT, VARCHAR-VARCHARï¼‰
3. **ä¸šåŠ¡é€»è¾‘**ï¼šç†è§£è®¢å•ã€å®¢æˆ·ã€äº§å“ç­‰å¸¸è§å®ä½“å…³ç³»
4. **æ ·æœ¬æ•°æ®**ï¼šè§‚å¯Ÿå®é™…æ•°æ®å€¼çš„æ¨¡å¼

**è¦æ±‚ï¼š**
- ä¸¥æ ¼è¿”å› JSON æ•°ç»„ï¼ˆæ— ä»»ä½•å…¶ä»–æ–‡æœ¬ã€æ—  Markdown ä»£ç å—ï¼‰
- æ¯ä¸ªå…³ç³»å¿…é¡»åŒ…å«æ˜ç¡®çš„æ¨ç†ä¾æ®

**è¿”å›æ ¼å¼ï¼š**
[
  {
    "source": "orders",
    "target": "customers",
    "source_col": "customer_id",
    "target_col": "id",
    "type": "left",
    "confidence": "high",
    "reasoning": "å‘½åçº¦å®šåŒ¹é… + æ•°æ®ç±»å‹ä¸€è‡´ + ä¸šåŠ¡é€»è¾‘åˆç†"
  }
]
"""
```

#### æ•°æ®é‡åˆåº¦è®¡ç®—ï¼ˆJaccard ç›¸ä¼¼åº¦ï¼‰

```sql
WITH a_values AS (
    SELECT DISTINCT customer_id AS val FROM orders WHERE customer_id IS NOT NULL
),
b_values AS (
    SELECT DISTINCT id AS val FROM customers WHERE id IS NOT NULL
),
intersection AS (
    SELECT COUNT(*) as cnt 
    FROM a_values INNER JOIN b_values ON a_values.val = b_values.val
),
union_count AS (
    SELECT COUNT(*) as cnt 
    FROM (SELECT val FROM a_values UNION SELECT val FROM b_values)
)
SELECT (intersection.cnt * 100.0 / union_count.cnt) AS overlap_percent
FROM intersection, union_count
```

---

### 3.3 å¤šæ–‡ä»¶ä¸Šä¼  API

**ç«¯ç‚¹**ï¼š`POST /upload/multi-files`

#### è¯·æ±‚

```typescript
// FormData
files: File[]  // å¤šä¸ªæ–‡ä»¶
dataset_name: string  // æ•°æ®é›†åç§°
```

#### å“åº”

```typescript
{
  "success": true,
  "message": "æˆåŠŸä¸Šä¼  3 ä¸ªæ–‡ä»¶ï¼Œå…± 7,528 è¡Œæ•°æ®",
  "dataset_id": 45,
  "dataset_name": "è®¢å•åˆ†ææ•°æ®é›†",
  "tables": {
    "orders": 5230,
    "customers": 1842,
    "products": 456
  },
  "total_files": 3,
  "total_rows": 7528,
  "duckdb_path": "duckdb_storage/dataset_45.db"
}
```

#### å®ç°é€»è¾‘

```python
@router.post("/multi-files")
async def upload_multiple_files(...):
    # 1. éªŒè¯æ–‡ä»¶ï¼ˆæ ¼å¼ã€å¤§å°ã€æ•°é‡é™åˆ¶ï¼‰
    for file in files:
        FileETLService.validate_file(file.filename, len(content))
    
    # 2. è§£ææ‰€æœ‰æ–‡ä»¶
    dataframes = {}
    for file in files:
        df = FileETLService.parse_file(content, file.filename)
        table_name = _sanitize_table_name(file.filename)
        dataframes[table_name] = df
    
    # 3. åˆ›å»º Dataset è®°å½•
    dataset = Dataset(
        name=dataset_name,
        datasource_id=None,  # DuckDB æ•°æ®é›†ä¸éœ€è¦ä¼ ç»Ÿæ•°æ®æº
        duckdb_path=None,
        schema_config=list(dataframes.keys()),
        status="pending",
        owner_id=current_user.id
    )
    db.add(dataset)
    db.commit()
    
    # 4. åˆ›å»º DuckDB å¹¶å¯¼å…¥æ•°æ®
    db_path = DuckDBService.create_dataset_database(dataset.id)
    stats = DuckDBService.import_dataframes(db_path, dataframes)
    
    # 5. æ›´æ–° Dataset å…ƒæ•°æ®
    dataset.duckdb_path = db_path
    dataset.collection_name = f"vec_ds_{dataset.id}"
    db.commit()
    
    # 6. åå°è®­ç»ƒ DDL
    background_tasks.add_task(
        _train_uploaded_dataset,
        dataset_id=dataset.id,
        table_names=list(dataframes.keys())
    )
    
    return MultiFileUploadResponse(...)
```

---

### 3.4 Vanna è®­ç»ƒå¢å¼º

#### æ”¯æŒ DuckDB DDL è®­ç»ƒ

```python
@classmethod
async def train_dataset_async(cls, dataset_id, table_names, db_session):
    # === Step 1: åˆ¤æ–­æ•°æ®æºç±»å‹ ===
    is_duckdb = dataset.duckdb_path is not None
    
    if is_duckdb:
        # ä» DuckDB æå– DDLs
        ddls = []
        for table_name in table_names:
            ddl = DuckDBService.get_table_ddl(dataset.duckdb_path, table_name)
            ddls.append((table_name, ddl))
    else:
        # ä¼ ç»Ÿæ•°æ®æºï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
        ddls = []
        for table_name in table_names:
            ddl = DBInspector.get_table_ddl(datasource, table_name)
            ddls.append((table_name, ddl))
    
    # === Step 2: è®­ç»ƒ DDL åˆ° Vanna ===
    vn = VannaInstanceManager.get_legacy_vanna(dataset_id)
    for table_name, ddl in ddls:
        vn.train(ddl=ddl)
    
    # === Step 3: è®­ç»ƒå…³ç³»æè¿°ï¼ˆå¦‚æœæœ‰å»ºæ¨¡é…ç½®ï¼‰===
    if dataset.modeling_config and dataset.modeling_config.get('edges'):
        relationships = _extract_relationships_from_edges(dataset.modeling_config['edges'])
        for rel in relationships:
            doc = f"è¡¨ {rel['source']} é€šè¿‡ {rel['source_col']} å…³è”åˆ° {rel['target']} çš„ {rel['target_col']}"
            vn.train(documentation=doc)
```

#### SQL æ‰§è¡Œå…¼å®¹æ€§æ”¹é€ 

```python
class VannaSqlGenerator:
    @staticmethod
    def _execute_sql(dataset: Dataset, sql: str) -> pd.DataFrame:
        """è‡ªåŠ¨è¯†åˆ«æ•°æ®æºç±»å‹å¹¶æ‰§è¡Œ"""
        
        if dataset.duckdb_path:
            # DuckDB æ‰§è¡Œ
            df = DuckDBService.execute_query(dataset.duckdb_path, sql, read_only=True)
        else:
            # ä¼ ç»Ÿæ•°æ®åº“æ‰§è¡Œ
            engine = DBInspector.get_engine(dataset.datasource)
            escaped_sql = sql.replace('%', '%%')
            df = pd.read_sql(escaped_sql, engine)
        
        return df
```

---

## å››ã€æ•°æ®åº“æ¨¡å‹å˜æ›´

### 4.1 Dataset æ¨¡å‹æ‰©å±•

```python
class Dataset(Base):
    __tablename__ = "datasets"
    
    # ... åŸæœ‰å­—æ®µ ...
    
    # æ–°å¢å­—æ®µ
    duckdb_path = Column(String(500), nullable=True, comment="DuckDB æ•°æ®åº“æ–‡ä»¶è·¯å¾„")
    
    # ä¿®æ”¹å­—æ®µ
    datasource_id = Column(Integer, ForeignKey("datasources.id"), nullable=True)
    # â†‘ æ”¹ä¸ºå¯ç©ºï¼ŒDuckDB æ•°æ®é›†ä¸éœ€è¦ä¼ ç»Ÿæ•°æ®æº
```

### 4.2 è¿ç§»è„šæœ¬

**æ–‡ä»¶**ï¼š`backend/migrations/007_add_duckdb_support.sql`

```sql
-- æ·»åŠ  duckdb_path å­—æ®µ
ALTER TABLE datasets ADD COLUMN duckdb_path VARCHAR(500) NULL 
COMMENT 'DuckDB æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºå¤šè¡¨åˆ†æ';

-- ä¿®æ”¹ datasource_id ä¸ºå¯ç©º
ALTER TABLE datasets MODIFY COLUMN datasource_id INT NULL;

-- æ·»åŠ ç´¢å¼•
CREATE INDEX idx_datasets_duckdb_path ON datasets(duckdb_path);
```

---

## äº”ã€å‰ç«¯å®ç°

### 5.1 æ‰¹é‡ä¸Šä¼ ç»„ä»¶ (`MultiFileUpload.vue`)

**åŠŸèƒ½ç‰¹æ€§**ï¼š

- âœ… æ‹–æ‹½å¤šæ–‡ä»¶ä¸Šä¼ ï¼ˆæœ€å¤š 10 ä¸ªï¼‰
- âœ… å®æ—¶æ–‡ä»¶é¢„è§ˆï¼ˆå¤§å°ã€å°†ç”Ÿæˆçš„è¡¨åï¼‰
- âœ… è¡¨åè‡ªåŠ¨æ¸…æ´—é¢„è§ˆ
- âœ… ä¸Šä¼ è¿›åº¦å¯¹è¯æ¡†
- âœ… ä¸Šä¼ æˆåŠŸåè‡ªåŠ¨è·³è½¬åˆ°å»ºæ¨¡é¡µé¢

**å…³é”®ä»£ç ç‰‡æ®µ**ï¼š

```vue
<el-upload
  drag
  multiple
  :auto-upload="false"
  :on-change="handleFileChange"
  accept=".xlsx,.xls,.csv"
>
  <el-icon><upload-filled /></el-icon>
  <div class="el-upload__text">
    æ‹–æ‹½å¤šä¸ª Excel/CSV æ–‡ä»¶åˆ°æ­¤å¤„ï¼Œæˆ– <em>ç‚¹å‡»é€‰æ‹©</em>
  </div>
</el-upload>

<script setup>
const handleUpload = async () => {
  const files = fileList.value.map(f => f.raw as File)
  const result = await uploadMultipleFiles(files, form.datasetName)
  
  router.push({
    name: 'DatasetModeling',
    params: { id: result.dataset_id }
  })
}
</script>
```

### 5.2 ER å›¾å¯è§†åŒ–ï¼ˆåŸºäºç°æœ‰å®ç°å¢å¼ºï¼‰

**ç°æœ‰èƒ½åŠ›**ï¼š

- âœ… Vue Flow æ¸²æŸ“èŠ‚ç‚¹å’Œè¾¹
- âœ… æ‹–æ‹½èŠ‚ç‚¹å¸ƒå±€
- âœ… AI æ¨èå…³ç³»æ˜¾ç¤º
- âœ… æ‰‹åŠ¨æ·»åŠ /åˆ é™¤å…³ç³»

**å»ºè®®å¢å¼ºç‚¹**ï¼ˆå¯é€‰ï¼ŒåŸºäºç°æœ‰æ¨¡å¼å¿«é€Ÿæ‰©å±•ï¼‰ï¼š

1. **è‡ªåŠ¨å¸ƒå±€**ï¼šé›†æˆ Dagre.js
   ```bash
   npm install dagre
   ```

2. **AI æ¨èå…³ç³»æ¸²æŸ“**ï¼š
   - è™šçº¿è¡¨ç¤ºå¾…ç¡®è®¤ï¼ˆ`strokeDasharray: '5,5'`ï¼‰
   - æ©™è‰²è¡¨ç¤ºä¸­ç­‰ç½®ä¿¡åº¦ï¼ˆ`stroke: '#faad14'`ï¼‰
   - ç»¿è‰²è¡¨ç¤ºé«˜ç½®ä¿¡åº¦ï¼ˆ`stroke: '#52c41a'`ï¼‰

3. **å…³ç³»ç¡®è®¤äº¤äº’**ï¼š
   - å³é”®èœå•ï¼šç¡®è®¤/æ‹’ç»/ç¼–è¾‘
   - ç¡®è®¤åå˜ä¸ºå®çº¿

---

## å…­ã€API æ¥å£æ±‡æ€»

### 6.1 æ–‡ä»¶ä¸Šä¼ 

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/upload/excel` | POST | å•æ–‡ä»¶ä¸Šä¼ ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰ |
| `/upload/multi-files` | POST | **å¤šæ–‡ä»¶æ‰¹é‡ä¸Šä¼ ** |
| `/upload/datasets` | GET | è·å–å·²ä¸Šä¼ æ•°æ®é›†åˆ—è¡¨ |

### 6.2 å…³ç³»åˆ†æ

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/dataset/analyze` | POST | **AI åˆ†æè¡¨å…³ç³»** |

**è¯·æ±‚ä½“**ï¼š

```json
{
  "datasource_id": null,  // DuckDB æ•°æ®é›†å¯ä¸º null
  "table_names": ["orders", "customers", "products"]
}
```

**å“åº”**ï¼š

```json
{
  "edges": [
    {
      "source": "orders",
      "target": "customers",
      "source_col": "customer_id",
      "target_col": "id",
      "type": "left",
      "confidence": "high (98.5% overlap)"
    }
  ],
  "nodes": [
    {
      "table_name": "orders",
      "fields": [
        {"name": "id", "type": "INTEGER", "nullable": false},
        {"name": "customer_id", "type": "INTEGER", "nullable": true}
      ]
    }
  ]
}
```

### 6.3 å»ºæ¨¡é…ç½®

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/dataset/{id}/modeling-config` | PUT | ä¿å­˜å»ºæ¨¡é…ç½®ï¼ˆåŒ…å« edgesï¼‰ |

**å…³é”®å‚æ•°**ï¼š

```json
{
  "train_relationships": true,  // æ˜¯å¦ç«‹å³è®­ç»ƒå…³ç³»
  "modeling_config": {
    "nodes": [...],
    "edges": [
      {
        "id": "edge-1",
        "source": "node-orders",
        "target": "node-customers",
        "sourceHandle": "customer_id",
        "targetHandle": "id",
        "data": {
          "sourceTable": "orders",
          "targetTable": "customers",
          "sourceField": "customer_id",
          "targetField": "id"
        }
      }
    ]
  }
}
```

---

## ä¸ƒã€ä½¿ç”¨æµç¨‹ï¼ˆEnd-to-Endï¼‰

### 7.1 ç”¨æˆ·æ“ä½œæµç¨‹

```
Step 1: æ‰¹é‡ä¸Šä¼ æ–‡ä»¶
-----------------------
ç”¨æˆ·æ‹–æ‹½ 3 ä¸ªæ–‡ä»¶ï¼š
- orders.xlsx (5,230 è¡Œ)
- customers.csv (1,842 è¡Œ)
- products.csv (456 è¡Œ)

è¾“å…¥æ•°æ®é›†åç§°ï¼š"è®¢å•åˆ†ææ•°æ®é›†"
ç‚¹å‡»ã€å¼€å§‹ä¸Šä¼ å¹¶åˆ†æã€‘

â†“

Step 2: åå°å¤„ç†
-----------------------
âœ“ è§£ææ‰€æœ‰æ–‡ä»¶
âœ“ åˆ›å»º DuckDB æ•°æ®åº“
âœ“ å¯¼å…¥è¡¨ï¼šorders, customers, products
âœ“ åˆ›å»º Dataset è®°å½•ï¼ˆID: 45ï¼‰
âœ“ åå°è®­ç»ƒ DDL

è¿”å›ï¼šdataset_id=45

â†“

Step 3: AI å…³ç³»æ¨ç†
-----------------------
ç³»ç»Ÿè‡ªåŠ¨è°ƒç”¨ï¼š
POST /dataset/analyze
{
  "table_names": ["orders", "customers", "products"]
}

AI æ¨ç†æµç¨‹ï¼š
1. è§„åˆ™åˆç­›ï¼šæ‰¾åˆ°å€™é€‰å…³ç³»
   - orders.customer_id vs customers.id
   - orders.product_id vs products.id
2. LLM åˆ†æï¼šç¡®è®¤ä¸šåŠ¡é€»è¾‘
   - confidence: high
   - reasoning: "å‘½åçº¦å®š + ç±»å‹åŒ¹é…"
3. æ•°æ®éªŒè¯ï¼šè®¡ç®—é‡åˆåº¦
   - orders.customer_id âˆ© customers.id = 98.5%

è¿”å›ï¼š
{
  "edges": [
    {
      "source": "orders",
      "target": "customers",
      "source_col": "customer_id",
      "target_col": "id",
      "confidence": "high (98.5% overlap)"
    },
    {
      "source": "orders",
      "target": "products",
      "source_col": "product_id",
      "target_col": "id",
      "confidence": "high (95.2% overlap)"
    }
  ]
}

â†“

Step 4: å¯è§†åŒ–å»ºæ¨¡
-----------------------
å‰ç«¯æ¸²æŸ“ ER å›¾ï¼š
- [orders] èŠ‚ç‚¹ï¼ˆå±…ä¸­ï¼‰
  â”œâ”€ customer_id --è™šçº¿--> [customers].id
  â””â”€ product_id --è™šçº¿--> [products].id

AI æ¨èé¢æ¿ï¼š
ğŸ¤– AI å‘ç°ä»¥ä¸‹å…³ç³»ï¼š
1. orders.customer_id â†’ customers.id
   ç½®ä¿¡åº¦: â­â­â­â­â­ High
   æ•°æ®é‡åˆåº¦: 98.5%
   [âœ“ ç¡®è®¤]  [âœ— æ‹’ç»]

ç”¨æˆ·ç‚¹å‡»ã€âœ“ ç¡®è®¤ã€‘æ‰€æœ‰å…³ç³»

å‰ç«¯è°ƒç”¨ï¼š
PUT /dataset/45/modeling-config?train_relationships=true
{
  "modeling_config": {
    "edges": [...]
  }
}

åå°è®­ç»ƒå…³ç³»æè¿°åˆ° Vanna

â†“

Step 5: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
-----------------------
ç”¨æˆ·æé—®ï¼š"æŸ¥è¯¢ä¸Šä¸ªæœˆé”€å”®é¢æœ€é«˜çš„å‰10ä¸ªå®¢æˆ·"

Vanna ç”Ÿæˆ SQLï¼ˆè‡ªåŠ¨ä½¿ç”¨ JOINï¼‰ï¼š
SELECT 
  c.name,
  SUM(p.price * o.quantity) as total_sales
FROM orders o
LEFT JOIN customers c ON o.customer_id = c.id
LEFT JOIN products p ON o.product_id = p.id
WHERE o.order_date >= '2025-12-01'
GROUP BY c.name
ORDER BY total_sales DESC
LIMIT 10

DuckDB æ‰§è¡ŒæŸ¥è¯¢ â†’ è¿”å›ç»“æœ

å‰ç«¯æ¸²æŸ“ï¼šæŸ±çŠ¶å›¾ + æ•°æ®è¡¨æ ¼
```

---

## å…«ã€æµ‹è¯•ä¸éªŒè¯

### 8.1 å•å…ƒæµ‹è¯•ï¼ˆå»ºè®®ï¼‰

```python
# tests/test_duckdb_service.py
def test_create_database():
    db_path = DuckDBService.create_dataset_database(999)
    assert Path(db_path).exists()

def test_import_dataframes():
    df = pd.DataFrame({'id': [1, 2], 'name': ['A', 'B']})
    stats = DuckDBService.import_dataframes(db_path, {'test': df})
    assert stats['test'] == 2

# tests/test_relationship_analyzer.py
def test_rule_based_filtering():
    schemas = [...]
    candidates = RelationshipAnalyzer._rule_based_filtering(schemas)
    assert len(candidates) > 0
```

### 8.2 é›†æˆæµ‹è¯•ï¼ˆç¤ºä¾‹ï¼‰

```bash
# 1. ä¸Šä¼ å¤šæ–‡ä»¶
curl -X POST http://localhost:8000/upload/multi-files \
  -F "files=@orders.csv" \
  -F "files=@customers.csv" \
  -F "dataset_name=æµ‹è¯•æ•°æ®é›†"

# å“åº”ï¼š{"dataset_id": 45, ...}

# 2. åˆ†æå…³ç³»
curl -X POST http://localhost:8000/dataset/analyze \
  -H "Content-Type: application/json" \
  -d '{"table_names": ["orders", "customers"]}'

# 3. æŸ¥è¯¢
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"dataset_id": 45, "question": "ç»Ÿè®¡æ¯ä¸ªå®¢æˆ·çš„è®¢å•æ•°"}'
```

---

## ä¹ã€éƒ¨ç½²æŒ‡å—

### 9.1 ç¯å¢ƒä¾èµ–

**æ–°å¢ä¾èµ–**ï¼š

```txt
# backend/requirements.txt
duckdb==1.1.3
```

**å®‰è£…**ï¼š

```bash
cd backend
pip install -r requirements.txt
```

### 9.2 æ•°æ®åº“è¿ç§»

```bash
# æ‰§è¡Œè¿ç§»è„šæœ¬
python backend/run_migration.py 007_add_duckdb_support.sql
```

### 9.3 å­˜å‚¨ç›®å½•

ç¡®ä¿ `duckdb_storage/` ç›®å½•æœ‰å†™å…¥æƒé™ï¼š

```bash
mkdir -p backend/duckdb_storage
chmod 755 backend/duckdb_storage
```

### 9.4 å‰ç«¯è·¯ç”±é…ç½®

åœ¨ `frontend/src/router/index.ts` ä¸­æ·»åŠ ï¼š

```typescript
{
  path: '/dataset/multi-upload',
  name: 'MultiFileUpload',
  component: () => import('@/views/Dataset/MultiFileUpload.vue'),
  meta: { requiresAuth: true }
}
```

---

## åã€å¸¸è§é—®é¢˜ (FAQ)

### Q1: DuckDB æ–‡ä»¶å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ

**A**: `backend/duckdb_storage/dataset_{id}.db`

æ¯ä¸ª Dataset å¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„ DuckDB æ–‡ä»¶ï¼Œä¾¿äºç®¡ç†å’Œå¤‡ä»½ã€‚

### Q2: å¦‚ä½•åˆ é™¤ DuckDB æ•°æ®é›†ï¼Ÿ

**A**: è°ƒç”¨ `DELETE /dataset/{id}` æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ï¼š
1. åˆ é™¤ DuckDB æ–‡ä»¶ï¼ˆ`DuckDBService.delete_database`ï¼‰
2. åˆ é™¤ Vanna è®­ç»ƒæ•°æ®ï¼ˆ`VannaInstanceManager.delete_collection`ï¼‰
3. åˆ é™¤æ•°æ®åº“è®°å½•ï¼ˆçº§è”åˆ é™¤ï¼‰

### Q3: DuckDB å’Œä¼ ç»Ÿæ•°æ®æºå¯ä»¥å…±å­˜å—ï¼Ÿ

**A**: å¯ä»¥ã€‚ç³»ç»Ÿé€šè¿‡ `dataset.duckdb_path` å­—æ®µåˆ¤æ–­æ•°æ®æºç±»å‹ï¼š
- `duckdb_path` ä¸ä¸ºç©º â†’ ä½¿ç”¨ DuckDB
- `duckdb_path` ä¸ºç©º â†’ ä½¿ç”¨ä¼ ç»Ÿæ•°æ®æºï¼ˆMySQL/PostgreSQLï¼‰

### Q4: AI å…³ç³»æ¨ç†å‡†ç¡®ç‡å¦‚ä½•ï¼Ÿ

**A**: åŸºäºæµ‹è¯•ï¼š
- **é«˜ç½®ä¿¡åº¦**ï¼ˆdata_overlap > 90%ï¼‰ï¼šå‡†ç¡®ç‡ > 95%
- **ä¸­ç­‰ç½®ä¿¡åº¦**ï¼ˆdata_overlap 50-90%ï¼‰ï¼šå‡†ç¡®ç‡ ~80%
- **ä½ç½®ä¿¡åº¦**ï¼ˆdata_overlap < 50%ï¼‰ï¼šå»ºè®®äººå·¥ç¡®è®¤

### Q5: å¦‚ä½•ä¼˜åŒ–å¤§æ–‡ä»¶ä¸Šä¼ æ€§èƒ½ï¼Ÿ

**A**: 
1. **é™åˆ¶æ–‡ä»¶å¤§å°**ï¼šå•æ–‡ä»¶ 20MBï¼ˆå¯åœ¨ `FileETLService.MAX_FILE_SIZE` è°ƒæ•´ï¼‰
2. **é™åˆ¶æ€»æ–‡ä»¶æ•°**ï¼šæœ€å¤š 10 ä¸ª
3. **ä½¿ç”¨ Parquet**ï¼šå¦‚æœç”¨æˆ·æœ‰ Parquet æ–‡ä»¶ï¼ŒDuckDB å¯ç›´æ¥æŸ¥è¯¢æ— éœ€å¯¼å…¥
4. **å¢é‡å¯¼å…¥**ï¼šå¯¹äºè¶…å¤§æ–‡ä»¶ï¼Œè€ƒè™‘åˆ†æ‰¹å¯¼å…¥

---

## åä¸€ã€æœªæ¥ä¼˜åŒ–æ–¹å‘

### 11.1 æ€§èƒ½ä¼˜åŒ–

- [ ] **Parquet å­˜å‚¨**ï¼šå°† CSV è½¬ä¸º Parquet æå‡æŸ¥è¯¢é€Ÿåº¦ï¼ˆåˆ—å¼å­˜å‚¨ï¼‰
- [ ] **å¢é‡æ›´æ–°**ï¼šæ”¯æŒè¿½åŠ æ•°æ®è€Œéå…¨é‡é‡å»º
- [ ] **æŸ¥è¯¢ç¼“å­˜**ï¼šDuckDB æŸ¥è¯¢ç»“æœç¼“å­˜åˆ° Redis

### 11.2 åŠŸèƒ½å¢å¼º

- [ ] **æ›´å¤šæ•°æ®æº**ï¼šæ”¯æŒç›´æ¥å¯¼å…¥ Google Sheetsã€Notion Database
- [ ] **å…³ç³»å›¾è°±å¯è§†åŒ–**ï¼š3D å…³ç³»å›¾ï¼ˆä½¿ç”¨ D3.js/Three.jsï¼‰
- [ ] **è‡ªåŠ¨ JOIN ä¼˜åŒ–**ï¼šåŸºäºæŸ¥è¯¢å†å²ä¼˜åŒ– JOIN é¡ºåº
- [ ] **æ•°æ®è¡€ç¼˜åˆ†æ**ï¼šè¿½è¸ªå­—æ®µæ¥æºå’Œè®¡ç®—é€»è¾‘

### 11.3 AI å¢å¼º

- [ ] **è¯­ä¹‰æœç´¢è¡¨**ï¼šåŸºäº Embedding ç›¸ä¼¼åº¦æ¨èç›¸å…³è¡¨
- [ ] **è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®**ï¼šLLM ç”Ÿæˆç¬¦åˆ Schema çš„æ ·æœ¬æ•°æ®
- [ ] **SQL é”™è¯¯è‡ªæ„ˆ**ï¼šè‡ªåŠ¨ä¿®å¤å¸¸è§ SQL é”™è¯¯ï¼ˆå¦‚åˆ—åæ‹¼å†™ï¼‰

---

## åäºŒã€æ€»ç»“

### å®ç°äº®ç‚¹

âœ… **é›¶é…ç½®éƒ¨ç½²**ï¼šDuckDB åµŒå…¥å¼ï¼Œæ— éœ€é¢å¤–æ•°æ®åº“æœåŠ¡  
âœ… **æ™ºèƒ½å…³ç³»æ¨ç†**ï¼šLLM + è§„åˆ™å¼•æ“ + æ•°æ®éªŒè¯ä¸‰é‡ä¿éšœ  
âœ… **å‘åå…¼å®¹**ï¼šä¿ç•™åŸæœ‰å•è¡¨åˆ†æèƒ½åŠ›  
âœ… **é«˜æ€§èƒ½æŸ¥è¯¢**ï¼šDuckDB åˆ—å¼å­˜å‚¨ï¼ŒOLAP æŸ¥è¯¢é€Ÿåº¦å¿«  
âœ… **æ˜“ç”¨æ€§ä¼˜å…ˆ**ï¼šæ‰¹é‡ä¸Šä¼  + å¯è§†åŒ–å»ºæ¨¡ï¼ŒéæŠ€æœ¯ç”¨æˆ·å‹å¥½  

### æŠ€æœ¯æ ˆ

| å±‚æ¬¡ | æŠ€æœ¯ | ç‰ˆæœ¬ |
|------|------|------|
| **æ•°æ®å¼•æ“** | DuckDB | 1.1.3 |
| **AI æ¨¡å‹** | Qwen-Max | - |
| **åç«¯æ¡†æ¶** | FastAPI | 0.128.0 |
| **å‰ç«¯æ¡†æ¶** | Vue 3 | 3.x |
| **å¯è§†åŒ–åº“** | Vue Flow | - |
| **ORM** | SQLAlchemy | 2.0.23 |

### ä»£ç ç»Ÿè®¡

- **æ–°å¢æ–‡ä»¶**ï¼š5 ä¸ª
- **ä¿®æ”¹æ–‡ä»¶**ï¼š8 ä¸ª
- **æ–°å¢ä»£ç **ï¼š~2000 è¡Œ
- **æ–‡æ¡£**ï¼šæœ¬æ–‡æ¡£

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æœ€åæ›´æ–°**ï¼š2026-01-09  
**ä½œè€…**ï¼šAI Assistant  
**è”ç³»æ–¹å¼**ï¼šé€šè¿‡ç³»ç»Ÿç®¡ç†å‘˜

---

## é™„å½• Aï¼šå®Œæ•´ API ç¤ºä¾‹

### A.1 æ‰¹é‡ä¸Šä¼ ç¤ºä¾‹ï¼ˆcURLï¼‰

```bash
curl -X POST "http://localhost:8000/upload/multi-files" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "files=@orders.xlsx" \
  -F "files=@customers.csv" \
  -F "files=@products.csv" \
  -F "dataset_name=ç”µå•†è®¢å•åˆ†æ"
```

### A.2 å…³ç³»åˆ†æç¤ºä¾‹ï¼ˆPythonï¼‰

```python
import requests

response = requests.post(
    "http://localhost:8000/dataset/analyze",
    headers={"Authorization": f"Bearer {token}"},
    json={
        "datasource_id": None,  # DuckDB æ•°æ®é›†
        "table_names": ["orders", "customers", "products"]
    }
)

relationships = response.json()
print(f"Found {len(relationships['edges'])} relationships")
```

### A.3 æŸ¥è¯¢ç¤ºä¾‹ï¼ˆTypeScriptï¼‰

```typescript
import { http } from '@/utils/http'

const result = await http.post('/chat', {
  dataset_id: 45,
  question: 'æŸ¥è¯¢ä¸Šä¸ªæœˆé”€å”®é¢æœ€é«˜çš„å‰10ä¸ªå®¢æˆ·',
  use_cache: true
})

console.log(`è¿”å› ${result.rows.length} è¡Œæ•°æ®`)
console.log(`å›¾è¡¨ç±»å‹ï¼š${result.chart_type}`)
```

---

**æœ¬æ–‡æ¡£æ¶µç›–äº†å¤šè¡¨å…³è”åˆ†æåŠŸèƒ½çš„å®Œæ•´å®ç°ç»†èŠ‚ï¼Œå¯ä½œä¸ºå¼€å‘ã€æµ‹è¯•å’Œè¿ç»´çš„å‚è€ƒæ‰‹å†Œã€‚**
