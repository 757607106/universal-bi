from sqlalchemy.orm import Session, selectinload
from sqlalchemy import select, inspect
from app.models.metadata import Dataset, TrainingLog
from app.core.config import settings
from app.core.redis import redis_service, generate_cache_key
from app.core.logger import get_logger
from app.services.db_inspector import DBInspector
from datetime import datetime, date
from decimal import Decimal
import re
import asyncio
import pandas as pd
import uuid
import json
import hashlib
import time
from openai import OpenAI as OpenAIClient

# Vanna 2.0 Imports
from vanna.core import Agent, ToolRegistry, ToolContext
from vanna.core.user import User, UserResolver, RequestContext
from vanna.core.llm.models import LlmRequest
from vanna.core.enhancer import DefaultLlmContextEnhancer
from vanna.integrations.openai import OpenAILlmService
from vanna.integrations.chromadb import ChromaAgentMemory
# Note: For SQL execution in Agent, we might need a Runner. 
# But for training (adding DDL), we just need Memory.

# Standard Vanna Imports for Mixin Pattern
from vanna.legacy.openai import OpenAI_Chat
from vanna.legacy.chromadb import ChromaDB_VectorStore

logger = get_logger(__name__)


# Custom Exception for Training Control
class TrainingStoppedException(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼šè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­"""
    pass


class VannaLegacy(ChromaDB_VectorStore, OpenAI_Chat):
    """
    Legacy Vanna class for SQL generation
    Combines ChromaDB vector store with OpenAI chat
    ä½¿ç”¨ä¼ å…¥çš„ chroma_client é¿å…é‡å¤åˆ›å»ºå¯¼è‡´çš„å†²çª
    """
    def __init__(self, config=None, chroma_client=None):
        # ä¿å­˜ config å¼•ç”¨ï¼Œä¾›çˆ¶ç±»æ–¹æ³•ä½¿ç”¨
        self.config = config or {}
        
        # === VannaBase éœ€è¦çš„å±æ€§ ===
        self.run_sql_is_set = False
        self.static_documentation = ""
        self.dialect = self.config.get("dialect", "SQL")
        self.language = self.config.get("language", None)
        self.max_tokens = self.config.get("max_tokens", 14000)
        
        # === ChromaDB_VectorStore éœ€è¦çš„å±æ€§ ===
        n_results = config.get('n_results', 10) if config else 10
        self.n_results_sql = config.get('n_results_sql', n_results) if config else n_results
        self.n_results_documentation = config.get('n_results_documentation', n_results) if config else n_results
        self.n_results_ddl = config.get('n_results_ddl', n_results) if config else n_results
        
        # å¦‚æœä¼ å…¥äº† chroma_clientï¼Œç›´æ¥ä½¿ç”¨è€Œä¸è°ƒç”¨çˆ¶ç±»çš„ __init__
        if chroma_client is not None:
            import chromadb.utils.embedding_functions as embedding_functions
            
            self.chroma_client = chroma_client
            collection_name = config.get('collection_name', 'vanna_collection') if config else 'vanna_collection'
            self.n_results = config.get('n_results', 10) if config else 10
            
            # åˆå§‹åŒ– embedding function
            self.embedding_function = embedding_functions.DefaultEmbeddingFunction()
            
            # åˆ›å»º vanna æ‰€éœ€çš„æ‰€æœ‰ collection
            self.ddl_collection = self.chroma_client.get_or_create_collection(
                name=f"{collection_name}_ddl",
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
            self.documentation_collection = self.chroma_client.get_or_create_collection(
                name=f"{collection_name}_documentation",
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
            self.sql_collection = self.chroma_client.get_or_create_collection(
                name=f"{collection_name}_sql",
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
        else:
            # å›é€€åˆ°åŸå§‹åˆå§‹åŒ–æ–¹å¼
            ChromaDB_VectorStore.__init__(self, config=config)
        
        # Initialize with custom OpenAI client
        if config and 'api_base' in config:
            self.client = OpenAIClient(
                api_key=config.get('api_key'),
                base_url=config.get('api_base')
            )
            # Store model name
            self.model = config.get('model', 'gpt-3.5-turbo')
        else:
            OpenAI_Chat.__init__(self, config=config)
    
    def submit_prompt(self, prompt, **kwargs):
        """
        Override submit_prompt to use custom client
        """
        if hasattr(self, 'client'):
            # Extract messages if provided in prompt
            if isinstance(prompt, list):
                messages = prompt
            else:
                messages = [
                    {"role": "system", "content": "You are a helpful assistant that generates SQL queries."},
                    {"role": "user", "content": str(prompt)}
                ]
            
            # Ensure all messages have valid content
            validated_messages = []
            for msg in messages:
                if isinstance(msg.get('content'), str) and msg['content'].strip():
                    validated_messages.append({
                        "role": msg['role'],
                        "content": msg['content']
                    })
            
            if not validated_messages:
                raise ValueError("No valid messages to send to LLM")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=validated_messages,
                **kwargs
            )
            return response.choices[0].message.content
        else:
            return super().submit_prompt(prompt, **kwargs)


class SimpleUserResolver(UserResolver):
    async def resolve_user(self, request_context: RequestContext) -> User:
        return User(id="admin", email="admin@example.com", group_memberships=['admin'])

class VannaManager:
    # Class-level ChromaDB client cache to prevent "different settings" error
    _chroma_clients = {}  # {collection_name: vanna_instance}
    _agent_instances = {}  # {collection_name: agent_instance}
    
    # Global ChromaDB persistent client (singleton)
    _global_chroma_client = None
    
    @classmethod
    async def clear_cache_async(cls, dataset_id: int) -> int:
        """
        æ¸…é™¤æŒ‡å®šæ•°æ®é›†çš„æ‰€æœ‰ç¼“å­˜æŸ¥è¯¢ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        æ¸…é™¤ç»“æœç¼“å­˜å’Œ SQL ç¼“å­˜
        å½“æ•°æ®é›†é‡æ–°è®­ç»ƒæˆ–æœ¯è¯­æ›´æ–°æ—¶ä½¿ç”¨
        
        Args:
            dataset_id: æ•°æ®é›†ID
            
        Returns:
            int: åˆ é™¤çš„é”®æ•°é‡, -1 è¡¨ç¤º Redis ä¸å¯ç”¨
        """
        try:
            total_deleted = 0
            
            # æ³¨æ„ï¼šç”±äº redis_service ä¸æä¾› keys() æ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åˆ é™¤
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å¼åŒ¹é…æ¥åˆ é™¤ç¼“å­˜
            # å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ç»´æŠ¤ä¸€ä¸ªç¼“å­˜é”®çš„é›†åˆä»¥ä¾¿æ‰¹é‡åˆ é™¤
            
            # æ–¹æ¡ˆï¼šä½¿ç”¨ Redis çš„åŸç”Ÿå®¢æˆ·ç«¯è¿›è¡Œæ‰¹é‡åˆ é™¤ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰
            # ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ç¼“å­˜é”®é›†åˆç®¡ç†
            if redis_service.redis_client:
                # 1. æ¸…é™¤ç»“æœç¼“å­˜
                result_pattern = f"bi:cache:{dataset_id}:*"
                async for key in redis_service.redis_client.scan_iter(match=result_pattern):
                    await redis_service.delete(key)
                    total_deleted += 1
                
                # 2. æ¸…é™¤ SQL ç¼“å­˜
                sql_pattern = f"bi:sql_cache:{dataset_id}:*"
                async for key in redis_service.redis_client.scan_iter(match=sql_pattern):
                    await redis_service.delete(key)
                    total_deleted += 1
                
                logger.info(f"Cleared {total_deleted} cache entries for dataset {dataset_id}")
            else:
                logger.warning("Redis unavailable, cannot clear cache")
                return -1
            
            return total_deleted
        except Exception as e:
            logger.error(f"Failed to clear cache for dataset {dataset_id}: {e}")
            return -1
    
    @classmethod
    def clear_cache(cls, dataset_id: int) -> int:
        """
        æ¸…é™¤æŒ‡å®šæ•°æ®é›†çš„æ‰€æœ‰ç¼“å­˜æŸ¥è¯¢ï¼ˆåŒæ­¥åŒ…è£…å™¨ï¼‰
        
        Args:
            dataset_id: æ•°æ®é›†ID
            
        Returns:
            int: åˆ é™¤çš„é”®æ•°é‡, -1 è¡¨ç¤º Redis ä¸å¯ç”¨
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(cls.clear_cache_async(dataset_id))
    
    @staticmethod
    def _get_global_chroma_client():
        """
        è·å–å…¨å±€å•ä¾‹ChromaDBå®¢æˆ·ç«¯ï¼Œç¡®ä¿æ‰€æœ‰å®ä¾‹ä½¿ç”¨ç›¸åŒé…ç½®
        """
        if VannaManager._global_chroma_client is None:
            import chromadb
            VannaManager._global_chroma_client = chromadb.PersistentClient(
                path=settings.CHROMA_PERSIST_DIR
            )
            logger.info(f"Initialized global ChromaDB client at {settings.CHROMA_PERSIST_DIR}")
        return VannaManager._global_chroma_client
    
    @staticmethod
    def get_legacy_vanna(dataset_id: int):
        """
        Initialize and return a Legacy Vanna instance for SQL generation.
        Uses the same configuration as Agent but with Legacy API.
        Reuses existing client if already created to avoid ChromaDB conflicts.
        """
        collection_name = f"vec_ds_{dataset_id}"
        
        # Return cached instance if exists
        if collection_name in VannaManager._chroma_clients:
            logger.debug(f"Reusing existing Vanna instance for collection {collection_name}")
            return VannaManager._chroma_clients[collection_name]
        
        # è·å–å…¨å±€å®¢æˆ·ç«¯å¹¶ä¼ å…¥ VannaLegacyï¼Œé¿å…å†…éƒ¨é‡å¤åˆ›å»º
        global_client = VannaManager._get_global_chroma_client()
        
        # Create new instance with shared global client
        vn = VannaLegacy(
            config={
                'api_key': settings.DASHSCOPE_API_KEY,
                'model': settings.QWEN_MODEL,
                'n_results': settings.CHROMA_N_RESULTS,
                'collection_name': collection_name,
                'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },
            chroma_client=global_client  # ä¼ å…¥å…¨å±€å®¢æˆ·ç«¯
        )
        
        # Enable data visibility for LLM to support intermediate_sql reasoning
        vn.allow_llm_to_see_data = True
        
        # Cache the instance
        VannaManager._chroma_clients[collection_name] = vn
        logger.info(f"Created new Vanna instance for collection {collection_name}")
        
        return vn

    @staticmethod
    def get_agent(dataset_id: int):
        """
        Initialize and return a configured Vanna Agent instance (Vanna 2.0).
        Reuses existing agent if already created to avoid ChromaDB conflicts.
        """
        collection_name = f"vec_ds_{dataset_id}"
        
        # Return cached agent if exists
        if collection_name in VannaManager._agent_instances:
            logger.debug(f"Reusing existing Agent instance for collection {collection_name}")
            return VannaManager._agent_instances[collection_name]
        
        # 1. LLM Service (Qwen via OpenAI compatible API)
        llm_service = OpenAILlmService(
            api_key=settings.DASHSCOPE_API_KEY,
            model=settings.QWEN_MODEL,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        
        # 2. Agent Memory (ChromaDB)
        # ChromaDBé…ç½®ä».envæ–‡ä»¶è¯»å–
        agent_memory = ChromaAgentMemory(
            persist_directory=settings.CHROMA_PERSIST_DIR,
            collection_name=collection_name
        )
        
        # 3. Tool Registry
        registry = ToolRegistry()
        # Register SQL tools if needed for querying. 
        # For now, we focus on training (memory population).
        
        # 4. Context Enhancer
        # Injects relevant DDL/Docs from memory into System Prompt
        enhancer = DefaultLlmContextEnhancer(agent_memory=agent_memory)
        
        # 5. Create Agent
        agent = Agent(
            llm_service=llm_service,
            tool_registry=registry,
            user_resolver=SimpleUserResolver(),
            agent_memory=agent_memory,
            llm_context_enhancer=enhancer
        )
        
        # Cache the agent instance
        VannaManager._agent_instances[collection_name] = agent
        logger.info(f"Created new Agent instance for collection {collection_name}")
        
        return agent

    @staticmethod
    def train_dataset(dataset_id: int, table_names: list[str], db_session: Session):
        """
        Train the dataset by extracting DDLs and feeding them to Vanna Memory.
        This wrapper handles the sync-to-async bridge.
        """
        # åœ¨è®­ç»ƒå‰ä¸»åŠ¨æ¸…ç†è¯¥æ•°æ®é›†çš„ç¼“å­˜ï¼Œé¿å… ChromaDB å®ä¾‹å†²çª
        collection_name = f"vec_ds_{dataset_id}"
        if collection_name in VannaManager._chroma_clients:
            logger.info(f"Clearing cached Vanna instance before training: {collection_name}")
            del VannaManager._chroma_clients[collection_name]
        if collection_name in VannaManager._agent_instances:
            logger.info(f"Clearing cached Agent instance before training: {collection_name}")
            del VannaManager._agent_instances[collection_name]
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
        loop.run_until_complete(VannaManager.train_dataset_async(dataset_id, table_names, db_session))

    @staticmethod
    def _checkpoint_and_check_interrupt(db_session: Session, dataset_id: int, progress: int, log_message: str):
        """
        æ£€æŸ¥ç‚¹ï¼šæ›´æ–°è¿›åº¦ã€è®°å½•æ—¥å¿—ã€æ£€æŸ¥ä¸­æ–­
        
        Args:
            db_session: æ•°æ®åº“ä¼šè¯
            dataset_id: æ•°æ®é›†ID
            progress: å½“å‰è¿›åº¦ (0-100)
            log_message: æ—¥å¿—æ¶ˆæ¯
            
        Raises:
            TrainingStoppedException: å¦‚æœçŠ¶æ€ä¸º paused
        """
        # 1. æ›´æ–°è¿›åº¦
        dataset = db_session.query(Dataset).filter(Dataset.id == dataset_id).first()
        if not dataset:
            raise ValueError(f"Dataset {dataset_id} not found")
        
        dataset.process_rate = progress
        
        # 2. è®°å½•æ—¥å¿—
        training_log = TrainingLog(
            dataset_id=dataset_id,
            content=f"[{progress}%] {log_message}"
        )
        db_session.add(training_log)
        db_session.commit()
        db_session.refresh(dataset)  # åˆ·æ–°ä»¥è·å–æœ€æ–°çŠ¶æ€
        
        logger.info(f"Checkpoint: Dataset {dataset_id} progress {progress}% - {log_message}")
        
        # 3. æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
        if dataset.status == "paused":
            logger.warning(f"Training interrupted by user for dataset {dataset_id}")
            raise TrainingStoppedException(f"è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ (Dataset {dataset_id})")
    
    @staticmethod
    def delete_collection(dataset_id: int):
        """
        åˆ é™¤ Vanna/ChromaDB ä¸­æŒ‡å®š dataset çš„ collection
        ç”¨äºæ¸…ç†å‘é‡åº“æ•°æ®
        
        Args:
            dataset_id: æ•°æ®é›†ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        collection_name = f"vec_ds_{dataset_id}"
        
        try:
            # 1. ä»ç¼“å­˜ä¸­ç§»é™¤
            if collection_name in VannaManager._chroma_clients:
                del VannaManager._chroma_clients[collection_name]
                logger.info(f"Removed Vanna instance from cache: {collection_name}")
            
            if collection_name in VannaManager._agent_instances:
                del VannaManager._agent_instances[collection_name]
                logger.info(f"Removed Agent instance from cache: {collection_name}")
            
            # 2. åˆ é™¤ ChromaDB collectionï¼ˆä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯ï¼‰
            chroma_client = VannaManager._get_global_chroma_client()
            
            try:
                chroma_client.delete_collection(name=collection_name)
                logger.info(f"Successfully deleted ChromaDB collection: {collection_name}")
            except Exception as e:
                # Collection ä¸å­˜åœ¨ä¹Ÿè§†ä¸ºæˆåŠŸ
                if "does not exist" in str(e).lower():
                    logger.info(f"Collection {collection_name} does not exist, nothing to delete")
                else:
                    logger.error(f"Failed to delete collection {collection_name}: {e}")
                    raise
            
            # 3. æ¸…ç†ç¼“å­˜
            VannaManager.clear_cache(dataset_id)
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to delete collection for dataset {dataset_id}: {e}")
            return False

    @staticmethod
    def get_training_data(dataset_id: int, page: int = 1, page_size: int = 20, type_filter: str = None) -> dict:
        """
        ä» ChromaDB collection ä¸­è·å–è®­ç»ƒæ•°æ®ï¼ˆQAå¯¹ã€DDLã€æ–‡æ¡£ç­‰ï¼‰
        æ”¯æŒåˆ†é¡µæŸ¥è¯¢ï¼Œå…¼å®¹æ—§ç‰ˆå’Œæ–°ç‰ˆå­˜å‚¨æ ¼å¼
        
        Args:
            dataset_id: æ•°æ®é›†ID
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            page_size: æ¯é¡µæ•°é‡
            type_filter: ç±»å‹ç­›é€‰ï¼Œå¯é€‰å€¼: 'ddl', 'sql', 'documentation', None(å…¨éƒ¨)
            
        Returns:
            dict: {
                'total': int,  # æ€»æ•°
                'items': [     # è®­ç»ƒæ•°æ®åˆ—è¡¨
                    {
                        'id': str,
                        'question': str,
                        'sql': str,
                        'training_data_type': str,  # 'sql', 'ddl', 'documentation'
                        'created_at': str or None
                    }
                ],
                'page': int,
                'page_size': int
            }
        """
        collection_name = f"vec_ds_{dataset_id}"
        
        try:
            # è·å–å…¨å±€ ChromaDB å®¢æˆ·ç«¯
            chroma_client = VannaManager._get_global_chroma_client()
            
            all_items = []
            
            # æ–¹æ¡ˆ 1ï¼šæ–°ç‰ˆåˆ†å¼€å­˜å‚¨çš„ collection
            # æ ¹æ® type_filter å†³å®šæŸ¥è¯¢å“ªäº› collection
            if type_filter and type_filter != 'all':
                collection_configs = [(f"{collection_name}_{type_filter}", type_filter)]
            else:
                collection_configs = [
                    (f"{collection_name}_ddl", 'ddl'),
                    (f"{collection_name}_documentation", 'documentation'),
                    (f"{collection_name}_sql", 'sql'),
                ]
            
            for coll_name, data_type in collection_configs:
                try:
                    collection = chroma_client.get_collection(name=coll_name)
                    result = collection.get(include=['documents', 'metadatas'])
                    
                    ids = result.get('ids', [])
                    documents = result.get('documents', [])
                    metadatas = result.get('metadatas', [])
                    
                    for i, doc_id in enumerate(ids):
                        document = documents[i] if i < len(documents) else ''
                        metadata = metadatas[i] if i < len(metadatas) and metadatas[i] is not None else {}
                        
                        # å¤„ç† metadata ä¸­çš„ questionï¼ˆå¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²ï¼‰
                        question = metadata.get('question', '')
                        if isinstance(question, str) and question.startswith('{'):
                            try:
                                import json
                                parsed = json.loads(question)
                                question = parsed.get('question', '')
                            except:
                                pass
                        
                        # å¤„ç† documentï¼ˆsql å¯èƒ½åœ¨ document æˆ– metadata ä¸­ï¼‰
                        sql = document
                        if isinstance(sql, str) and sql.startswith('{'):
                            try:
                                import json
                                parsed = json.loads(sql)
                                question = parsed.get('question', question)
                                sql = parsed.get('sql', sql)
                            except:
                                pass
                        
                        training_data_type = data_type
                        
                        # DDL ç±»å‹ï¼šä½¿ç”¨è¡¨åä½œä¸º question
                        if training_data_type == 'ddl' and not question:
                            import re
                            table_match = re.search(r'CREATE TABLE\s+`?(\w+)`?', sql, re.IGNORECASE)
                            if table_match:
                                question = table_match.group(1)
                            else:
                                question = "æ•°æ®åº“è¡¨ç»“æ„å®šä¹‰"
                        
                        # æ–‡æ¡£ç±»å‹
                        if training_data_type == 'documentation' and not question:
                            if 'ä¸šåŠ¡æœ¯è¯­' in sql:
                                # æå–æœ¯è¯­åç§°
                                term_match = re.search(r'ä¸šåŠ¡æœ¯è¯­[ï¼š:]\s*(.+?)\n', sql)
                                if term_match:
                                    question = term_match.group(1)
                                else:
                                    question = "ä¸šåŠ¡æœ¯è¯­å®šä¹‰"
                            elif 'joined with' in sql.lower():
                                # æå–è¡¨å…³ç³»
                                relation_match = re.search(r'`(\w+)`.*?joined with.*?`(\w+)`', sql, re.IGNORECASE)
                                if relation_match:
                                    question = f"{relation_match.group(1)} ä¸ {relation_match.group(2)} çš„å…³ç³»"
                                else:
                                    question = "è¡¨å…³ç³»æè¿°"
                            else:
                                # å–æ–‡æ¡£å‰50å­—ç¬¦ä½œä¸ºæè¿°
                                question = sql[:50].strip()
                        
                        # SQL QA å¯¹
                        if training_data_type == 'sql' and not question:
                            question = metadata.get('text', 'ç¤ºä¾‹æŸ¥è¯¢')
                        
                        all_items.append({
                            'id': doc_id,
                            'question': question or 'æœªå‘½å',
                            'sql': sql,
                            'training_data_type': training_data_type,
                            'created_at': metadata.get('created_at') or metadata.get('timestamp')
                        })
                        
                except Exception as e:
                    if "does not exist" not in str(e).lower():
                        logger.warning(f"Failed to get data from collection {coll_name}: {e}")
                    continue
            
            # æ–¹æ¡ˆ 2ï¼šæ—§ç‰ˆå•ä¸€ collection æ ¼å¼ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            if len(all_items) == 0:
                try:
                    collection = chroma_client.get_collection(name=collection_name)
                    result = collection.get(include=['documents', 'metadatas'])
                    
                    ids = result.get('ids', [])
                    documents = result.get('documents', [])
                    metadatas = result.get('metadatas', [])
                    
                    for i, doc_id in enumerate(ids):
                        document = documents[i] if i < len(documents) else ''
                        metadata = metadatas[i] if i < len(metadatas) and metadatas[i] is not None else {}
                        
                        question = metadata.get('question', '')
                        sql = document
                        
                        # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹
                        training_data_type = 'sql'  # é»˜è®¤
                        if 'CREATE TABLE' in document.upper() or 'CREATE VIEW' in document.upper():
                            training_data_type = 'ddl'
                        elif question is None or question == '':
                            if 'ä¸šåŠ¡' in document or 'joined with' in document.lower():
                                training_data_type = 'documentation'
                        
                        # åº”ç”¨ç±»å‹ç­›é€‰
                        if type_filter and type_filter != 'all' and training_data_type != type_filter:
                            continue
                        
                        # DDL ç±»å‹ï¼šä½¿ç”¨è¡¨åä½œä¸º question
                        if training_data_type == 'ddl' and not question:
                            import re
                            table_match = re.search(r'CREATE TABLE\s+`?(\w+)`?', sql, re.IGNORECASE)
                            if table_match:
                                question = f"è¡¨ç»“æ„: {table_match.group(1)}"
                            else:
                                question = "æ•°æ®åº“è¡¨ç»“æ„å®šä¹‰"
                        
                        # æ–‡æ¡£ç±»å‹
                        if training_data_type == 'documentation' and not question:
                            question = "ä¸šåŠ¡æ–‡æ¡£"
                        
                        # SQL QA å¯¹
                        if training_data_type == 'sql' and not question:
                            question = metadata.get('text', 'ç¤ºä¾‹æŸ¥è¯¢')
                        
                        all_items.append({
                            'id': doc_id,
                            'question': question,
                            'sql': sql,
                            'training_data_type': training_data_type,
                            'created_at': metadata.get('created_at') or metadata.get('timestamp')
                        })
                        
                except Exception as e:
                    if "does not exist" not in str(e).lower():
                        logger.warning(f"Failed to get data from legacy collection {collection_name}: {e}")
            
            # è®¡ç®—åˆ†é¡µ
            total = len(all_items)
            start = (page - 1) * page_size
            end = start + page_size
            items = all_items[start:end]
            
            logger.info(f"Retrieved {len(items)} training data items (page {page}/{(total + page_size - 1) // page_size if total > 0 else 1}) for dataset {dataset_id}")
            
            return {
                'total': total,
                'items': items,
                'page': page,
                'page_size': page_size
            }
            
        except Exception as e:
            logger.error(f"Failed to get training data for dataset {dataset_id}: {e}")
            raise ValueError(f"è·å–è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}")

    @staticmethod
    async def train_dataset_async(dataset_id: int, table_names: list[str], db_session: Session):
        """
        å¼‚æ­¥è®­ç»ƒæ•°æ®é›†ï¼Œæ”¯æŒè¿›åº¦æ›´æ–°å’Œä¸­æ–­æ§åˆ¶
        
        æµç¨‹ï¼š
        - Step 0-10%: åˆå§‹åŒ–ã€æ£€æŸ¥æ•°æ®åº“è¿æ¥ã€æå– DDL
        - Step 10-40%: è®­ç»ƒ DDL åˆ° Vanna
        - Step 40-80%: è®­ç»ƒæ–‡æ¡£/ä¸šåŠ¡æœ¯è¯­
        - Step 80-100%: ç”Ÿæˆç¤ºä¾‹ SQLQA å¯¹å¹¶è®­ç»ƒ
        
        Args:
            dataset_id: æ•°æ®é›†ID
            table_names: è¦è®­ç»ƒçš„è¡¨ååˆ—è¡¨
            db_session: æ•°æ®åº“ä¼šè¯
        """
        logger.info(f"Starting training for dataset {dataset_id} with tables: {table_names}")
        
        dataset = db_session.query(Dataset).filter(Dataset.id == dataset_id).first()
        if not dataset:
            logger.error(f"Dataset {dataset_id} not found")
            return
        
        try:
            # === Step 0: åˆå§‹åŒ– (0%) ===
            dataset.status = "training"
            dataset.process_rate = 0
            dataset.error_msg = None
            db_session.commit()
            
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 0, "è®­ç»ƒå¯åŠ¨")
            
            # === Step 1: æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œæå– DDL (0-10%) ===
            datasource = dataset.datasource
            if not datasource:
                raise ValueError("DataSource associated with dataset not found")
            
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 5, "æ£€æŸ¥æ•°æ®æºè¿æ¥")
            
            # æå– DDLs
            ddls = []
            for i, table_name in enumerate(table_names):
                try:
                    ddl = DBInspector.get_table_ddl(datasource, table_name)
                    ddls.append((table_name, ddl))
                    
                    # æ¯å¤„ç†ä¸€ä¸ªè¡¨ï¼Œæ›´æ–°ä¸€æ¬¡è¿›åº¦
                    progress = 5 + int((i + 1) / len(table_names) * 5)
                    VannaManager._checkpoint_and_check_interrupt(
                        db_session, dataset_id, progress, 
                        f"æå–è¡¨ DDL: {table_name} ({i+1}/{len(table_names)})"
                    )
                except Exception as e:
                    logger.warning(f"Failed to get DDL for {table_name}: {e}")
                    # ç»§ç»­å¤„ç†å…¶ä»–è¡¨
            
            if not ddls:
                raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•è¡¨çš„ DDL")
            
            # === Step 2: åˆå§‹åŒ– Vanna å®ä¾‹ (10%) ===
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 10, "åˆå§‹åŒ– Vanna å®ä¾‹")
            
            # ä½¿ç”¨ Legacy API è¿›è¡Œè®­ç»ƒ
            vn = VannaManager.get_legacy_vanna(dataset_id)
            
            # === Step 3: è®­ç»ƒ DDL (10-40%) ===
            for i, (table_name, ddl) in enumerate(ddls):
                if ddl:
                    vn.train(ddl=ddl)
                    
                    # æ›´æ–°è¿›åº¦
                    progress = 10 + int((i + 1) / len(ddls) * 30)
                    VannaManager._checkpoint_and_check_interrupt(
                        db_session, dataset_id, progress,
                        f"è®­ç»ƒ DDL: {table_name} ({i+1}/{len(ddls)})"
                    )
            
            # === Step 4: è®­ç»ƒæ–‡æ¡£/ä¸šåŠ¡æœ¯è¯­ (40-80%) ===
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 40, "å¼€å§‹è®­ç»ƒä¸šåŠ¡æœ¯è¯­")
            
            # è·å–å¹¶è®­ç»ƒä¸šåŠ¡æœ¯è¯­
            from app.models.metadata import BusinessTerm
            business_terms = db_session.query(BusinessTerm).filter(
                BusinessTerm.dataset_id == dataset_id
            ).all()
            
            if business_terms:
                for i, term in enumerate(business_terms):
                    doc_content = f"ä¸šåŠ¡æœ¯è¯­: {term.term}\nå®šä¹‰: {term.definition}"
                    vn.train(documentation=doc_content)
                    
                    progress = 40 + int((i + 1) / len(business_terms) * 20)
                    VannaManager._checkpoint_and_check_interrupt(
                        db_session, dataset_id, progress,
                        f"è®­ç»ƒä¸šåŠ¡æœ¯è¯­: {term.term} ({i+1}/{len(business_terms)})"
                    )
            else:
                VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 60, "æ— ä¸šåŠ¡æœ¯è¯­ï¼Œè·³è¿‡")
            
            # è®­ç»ƒè¡¨å…³ç³»æ–‡æ¡£
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 70, "ç”Ÿæˆè¡¨å…³ç³»æ–‡æ¡£")
            
            # ç”Ÿæˆè¡¨å…³ç³»æè¿°
            table_relationships_doc = f"""æ•°æ®åº“è¡¨ç»“æ„ï¼š
æœ¬æ•°æ®é›†åŒ…å«ä»¥ä¸‹è¡¨ï¼š{', '.join([name for name, _ in ddls])}

è¯·æ ¹æ®è¡¨åå’Œå­—æ®µåç”Ÿæˆ SQL æŸ¥è¯¢ã€‚
"""
            vn.train(documentation=table_relationships_doc)
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 80, "è¡¨å…³ç³»æ–‡æ¡£è®­ç»ƒå®Œæˆ")
            
            # === Step 5: ç”Ÿæˆç¤ºä¾‹ SQLQA å¯¹ (80-100%) ===
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 85, "ç”Ÿæˆç¤ºä¾‹ SQL æŸ¥è¯¢")
            
            # ä¸ºä¸»è¦è¡¨ç”ŸæˆåŸºæœ¬æŸ¥è¯¢ç¤ºä¾‹
            example_queries = []
            for table_name, _ in ddls[:3]:  # åªä¸ºå‰3ä¸ªè¡¨ç”Ÿæˆç¤ºä¾‹
                example_queries.append((
                    f"æŸ¥è¯¢ {table_name} è¡¨çš„æ‰€æœ‰æ•°æ®",
                    f"SELECT * FROM {table_name} LIMIT 100"
                ))
                example_queries.append((
                    f"ç»Ÿè®¡ {table_name} è¡¨çš„æ€»æ•°",
                    f"SELECT COUNT(*) as total FROM {table_name}"
                ))
            
            for i, (question, sql) in enumerate(example_queries):
                vn.train(question=question, sql=sql)
                
                progress = 85 + int((i + 1) / len(example_queries) * 15)
                VannaManager._checkpoint_and_check_interrupt(
                    db_session, dataset_id, progress,
                    f"è®­ç»ƒç¤ºä¾‹æŸ¥è¯¢ ({i+1}/{len(example_queries)})"
                )
            
            # === å®Œæˆ (100%) ===
            dataset.status = "completed"
            dataset.process_rate = 100
            dataset.last_train_at = datetime.utcnow()
            db_session.commit()
            
            VannaManager._checkpoint_and_check_interrupt(db_session, dataset_id, 100, "è®­ç»ƒå®Œæˆ")
            
            logger.info(f"Training completed successfully for dataset {dataset_id}")
            
            # æ¸…ç†ç¼“å­˜
            cleared = VannaManager.clear_cache(dataset_id)
            if cleared >= 0:
                logger.info(f"Cleared {cleared} cached queries after training dataset {dataset_id}")
            
        except TrainingStoppedException as e:
            # è¢«ç”¨æˆ·ä¸­æ–­
            logger.warning(f"Training stopped by user for dataset {dataset_id}: {e}")
            dataset.error_msg = str(e)
            db_session.commit()
            
            # è®°å½•ä¸­æ–­æ—¥å¿—
            training_log = TrainingLog(
                dataset_id=dataset_id,
                content=f"[{dataset.process_rate}%] è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­"
            )
            db_session.add(training_log)
            db_session.commit()
            
        except Exception as e:
            # è®­ç»ƒå¤±è´¥
            logger.error(f"Training failed for dataset {dataset_id}: {e}", exc_info=True)
            dataset.status = "failed"
            dataset.error_msg = str(e)
            db_session.commit()
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            training_log = TrainingLog(
                dataset_id=dataset_id,
                content=f"[{dataset.process_rate}%] è®­ç»ƒå¤±è´¥: {str(e)[:200]}"
            )
            db_session.add(training_log)
            db_session.commit()

    @staticmethod
    def train_term(dataset_id: int, term: str, definition: str, db_session: Session):
        """
        Train a business term by adding it to Vanna's documentation memory.
        Uses Legacy API for consistency with existing training approach.
        Clears cache after training to ensure fresh results.
        """
        try:
            # Get Legacy Vanna instance
            vn = VannaManager.get_legacy_vanna(dataset_id)
            
            # Format as documentation and train
            doc_content = f"ä¸šåŠ¡æœ¯è¯­: {term}\nå®šä¹‰: {definition}"
            vn.train(documentation=doc_content)
            
            logger.info(f"Successfully trained business term '{term}' for dataset {dataset_id}")
            
            # Clear cache after training new term
            cleared = VannaManager.clear_cache(dataset_id)
            if cleared >= 0:
                logger.info(f"Cleared {cleared} cached queries after training term '{term}'")
            
        except Exception as e:
            logger.error(f"Failed to train business term '{term}': {e}")
            raise ValueError(f"è®­ç»ƒé—®ç­”å¯¹å¤±è´¥: {str(e)}")
    
    @staticmethod
    def train_relationships(dataset_id: int, relationships: list[str], db_session: Session):
        """
        Train table relationships by adding them to Vanna's documentation memory.
        This enhances Vanna's ability to generate correct JOIN queries.
        
        Args:
            dataset_id: Dataset ID
            relationships: List of relationship descriptions (natural language)
            db_session: Database session
            
        Example relationships:
            [
                "The table `users` can be joined with table `orders` on the condition `users`.`id` = `orders`.`user_id`.",
                "The table `orders` can be joined with table `products` on the condition `orders`.`product_id` = `products`.`id`."
            ]
        """
        if not relationships or len(relationships) == 0:
            logger.info(f"No relationships to train for dataset {dataset_id}")
            return
        
        try:
            # Get Legacy Vanna instance
            vn = VannaManager.get_legacy_vanna(dataset_id)
            
            logger.info(f"Training {len(relationships)} relationships for dataset {dataset_id}")
            
            # Train each relationship as documentation
            for i, rel_desc in enumerate(relationships, 1):
                # Format as documentation
                doc_content = f"è¡¨å…³ç³»: {rel_desc}"
                vn.train(documentation=doc_content)
                logger.debug(f"Trained relationship {i}/{len(relationships)}: {rel_desc[:80]}...")
            
            logger.info(f"Successfully trained {len(relationships)} relationships for dataset {dataset_id}")
            
            # Clear cache after training relationships
            cleared = VannaManager.clear_cache(dataset_id)
            if cleared >= 0:
                logger.info(f"Cleared {cleared} cached queries after training relationships")
            
        except Exception as e:
            logger.error(f"Failed to train relationships: {e}")
            raise ValueError(f"è®­ç»ƒè¡¨å…³ç³»å¤±è´¥: {str(e)}")
    
    @staticmethod
    def train_qa(dataset_id: int, question: str, sql: str, db_session: Session):
        """
        Train a successful question-SQL pair from user feedback.
        This helps AI learn from correct examples and improve future responses.
        
        Args:
            dataset_id: Dataset ID
            question: User's question
            sql: Correct SQL for the question
            db_session: Database session
        
        Raises:
            ValueError: If training fails
        """
        try:
            # Get Legacy Vanna instance
            vn = VannaManager.get_legacy_vanna(dataset_id)
            
            # Train the Q-SQL pair
            vn.train(question=question, sql=sql)
            
            logger.info(f"Successfully trained Q-A pair for dataset {dataset_id}: '{question[:50]}...'")
            
            # Clear cache after training to ensure fresh results
            cleared = VannaManager.clear_cache(dataset_id)
            if cleared >= 0:
                logger.info(f"Cleared {cleared} cached queries after training Q-A pair")
            
        except Exception as e:
            logger.error(f"Failed to train Q-A pair for dataset {dataset_id}: {e}")
            raise ValueError(f"è®­ç»ƒé—®ç­”å¯¹å¤±è´¥: {str(e)}")

    @staticmethod
    async def generate_result(dataset_id: int, question: str, db_session: Session, use_cache: bool = True):
        """
        Generate SQL and execute it with intelligent multi-round dialogue (Auto-Reflection Loop).
        Now with Redis-based query caching for improved performance.
        
        Features:
        1. Redis-based SQL caching with 24-hour TTL
        2. LLM-driven intermediate SQL detection and execution
        3. Intelligent clarification generation when ambiguous
        4. Graceful text-only responses (no exceptions for non-SQL)
        5. LLM-powered friendly error messages on failures
        6. All responses in Chinese
        
        Args:
            dataset_id: Dataset ID
            question: User's question
            db_session: Database session
            use_cache: Whether to use cache (default: True)
            
        Returns:
            dict: Result with sql, columns, rows, chart_type, etc.
                  Includes 'is_cached' flag when result is from cache
        """
        execution_steps = []
        start_time = time.perf_counter()  # ğŸ”¥ å¼€å§‹è®¡æ—¶
        
        # è®°å½•è¯·æ±‚å¼€å§‹
        logger.info(
            "SQL generation started",
            dataset_id=dataset_id,
            question=question[:100],  # æˆªæ–­é—®é¢˜é•¿åº¦
            question_length=len(question),
            use_cache=use_cache
        )
        
        # === Step 0: SQL Cache Check ===
        # ç¼“å­˜ç­–ç•¥ï¼šå…ˆæ£€æŸ¥ SQL ç¼“å­˜ï¼Œå‘½ä¸­åé‡æ–°æ‰§è¡Œ SQL è·å–æœ€æ–°æ•°æ®
        if use_cache:
            cache_check_start = time.perf_counter()
            sql_cache_key = generate_cache_key("bi:sql_cache", dataset_id, question)
            
            try:
                cached_sql = await redis_service.get(sql_cache_key)
                if cached_sql:
                    cache_check_time = (time.perf_counter() - cache_check_start) * 1000
                    logger.info(
                        "SQL cache hit",
                        dataset_id=dataset_id,
                        cache_key=sql_cache_key[:50],
                        cache_check_time_ms=round(cache_check_time, 2)
                    )
                    execution_steps.append("SQLç¼“å­˜å‘½ä¸­")
                    
                    # å…³é”®ç‚¹ï¼šæ‹¿åˆ°ç¼“å­˜çš„ SQL åï¼Œé‡æ–°æ‰§è¡ŒæŸ¥è¯¢è·å–æœ€æ–°æ•°æ®
                    try:
                        # è·å– dataset å’Œ datasource
                        stmt = select(Dataset).options(selectinload(Dataset.datasource)).where(Dataset.id == dataset_id)
                        result = db_session.execute(stmt)
                        dataset = result.scalars().first()
                        
                        if not dataset or not dataset.datasource:
                            # å¦‚æœ dataset æˆ– datasource ä¸å­˜åœ¨ï¼Œè¿™ä¸ªç¼“å­˜å·²ç»æ— æ•ˆäº†
                            logger.warning(
                                "ç¼“å­˜å¤±æ•ˆ",
                                dataset_id=dataset_id,
                                reason="dataset or datasource not found"
                            )
                            await redis_service.delete(sql_cache_key)
                            execution_steps.append("ç¼“å­˜å·²å¤±æ•ˆï¼Œè¿›å…¥å¸¸è§„æµç¨‹")
                        else:
                            # é‡æ–°æ‰§è¡Œ SQL æŸ¥è¯¢
                            sql_exec_start = time.perf_counter()
                            engine = DBInspector.get_engine(dataset.datasource)
                            df = pd.read_sql(cached_sql, engine)
                            sql_exec_time = (time.perf_counter() - sql_exec_start) * 1000
                            
                            logger.info(
                                "SQL executed from cache",
                                dataset_id=dataset_id,
                                sql=cached_sql[:200],  # æˆªæ–­ SQL
                                row_count=len(df),
                                sql_exec_time_ms=round(sql_exec_time, 2)
                            )
                            execution_steps.append(f"é‡æ–°æ‰§è¡ŒæŸ¥è¯¢ï¼Œè¿”å› {len(df)} è¡Œ")
                            
                            # æ¨æ–­å›¾è¡¨ç±»å‹
                            chart_type = VannaManager._infer_chart_type(df)
                            
                            # åºåˆ—åŒ–æ•°æ®
                            cleaned_rows = VannaManager._serialize_dataframe(df)
                            
                            # ç”Ÿæˆä¸šåŠ¡åˆ†æï¼ˆå³ä½¿æ˜¯ç¼“å­˜çš„ SQLï¼Œä¹Ÿç”Ÿæˆæœ€æ–°çš„åˆ†æï¼‰
                            insight = None
                            if len(df) > 0:
                                try:
                                    execution_steps.append("æ­£åœ¨ç”Ÿæˆä¸šåŠ¡åˆ†æ...")
                                    insight = VannaManager.generate_data_insight(
                                        question=question,
                                        sql=cached_sql,
                                        df=df,
                                        dataset_id=dataset_id
                                    )
                                    execution_steps.append("ä¸šåŠ¡åˆ†æç”Ÿæˆå®Œæˆ")
                                    logger.info(f"Business insight generated from cache: {insight[:50]}...")
                                except Exception as insight_error:
                                    logger.warning(
                                        "ä¸šåŠ¡åˆ†æç”Ÿæˆå¤±è´¥",
                                        dataset_id=dataset_id,
                                        error=str(insight_error)
                                    )
                                    execution_steps.append("ä¸šåŠ¡åˆ†æç”Ÿæˆå¤±è´¥")
                                    insight = None
                            
                            total_time = (time.perf_counter() - start_time) * 1000
                            logger.info(
                                "Request completed (from cache)",
                                dataset_id=dataset_id,
                                total_time_ms=round(total_time, 2),
                                from_cache=True
                            )
                            
                            return {
                                "sql": cached_sql,
                                "columns": df.columns.tolist(),
                                "rows": cleaned_rows,
                                "chart_type": chart_type,
                                "summary": None,
                                "steps": execution_steps,
                                "is_cached": True,  # æ ‡è®°ä»ç¼“å­˜è¯»å–
                                "from_cache": True,  # å…¼å®¹æ—§å­—æ®µ
                                "insight": insight  # æ–°å¢ä¸šåŠ¡åˆ†æ
                            }
                    except Exception as e:
                        logger.warning(
                            "ç¼“å­˜ SQL æ‰§è¡Œå¤±è´¥",
                            dataset_id=dataset_id,
                            error=str(e)[:200],
                            cached_sql=cached_sql[:100]
                        )
                        execution_steps.append(f"ç¼“å­˜ SQL æ‰§è¡Œå¤±è´¥: {str(e)[:50]}ï¼Œè¿›å…¥å¸¸è§„æµç¨‹")
                        # åˆ é™¤æ— æ•ˆç¼“å­˜
                        await redis_service.delete(sql_cache_key)
                else:
                    logger.debug("ç¼“å­˜æœªå‘½ä¸­", dataset_id=dataset_id)
                    execution_steps.append("SQLç¼“å­˜æœªå‘½ä¸­")
            except Exception as e:
                logger.warning(f"SQL cache read failed: {e}. Proceeding without cache.")
                execution_steps.append(f"SQLç¼“å­˜è¯»å–å¤±è´¥: {str(e)[:50]}")
        
        try:
            # === Step A: Initial Setup ===
            # 1. Get Dataset with eager loading of datasource
            stmt = select(Dataset).options(selectinload(Dataset.datasource)).where(Dataset.id == dataset_id)
            result = db_session.execute(stmt)
            dataset = result.scalars().first()
            
            if not dataset:
                return {
                    "sql": None,
                    "columns": None,
                    "rows": None,
                    "chart_type": "clarification",
                    "answer_text": f"æ‰¾ä¸åˆ°æ•°æ®é›† ID: {dataset_id}ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨ã€‚",
                    "steps": ["æ•°æ®é›†æŸ¥æ‰¾å¤±è´¥"]
                }
            
            if not dataset.datasource:
                return {
                    "sql": None,
                    "columns": None,
                    "rows": None,
                    "chart_type": "clarification",
                    "answer_text": "æ•°æ®é›†æœªå…³è”æ•°æ®æºï¼Œè¯·å…ˆé…ç½®æ•°æ®æºã€‚",
                    "steps": ["æ•°æ®æºé…ç½®æ£€æŸ¥å¤±è´¥"]
                }

            # 2. Get Vanna instance and database engine
            vn = VannaManager.get_legacy_vanna(dataset_id)
            engine = DBInspector.get_engine(dataset.datasource)
            
            execution_steps.append("åˆå§‹åŒ–å®Œæˆ")

            # === Step B: Initial Generation ===
            llm_gen_start = time.perf_counter()
            try:
                llm_response = vn.generate_sql(question + " (è¯·ç”¨ä¸­æ–‡å›ç­”)")
                llm_gen_time = (time.perf_counter() - llm_gen_start) * 1000
                
                logger.info(
                    "LLM SQL generation completed",
                    dataset_id=dataset_id,
                    llm_gen_time_ms=round(llm_gen_time, 2),
                    response_length=len(llm_response)
                )
                execution_steps.append("LLM åˆå§‹å“åº”ç”Ÿæˆ")
                logger.debug(
                    "LLM å“åº”å†…å®¹",
                    dataset_id=dataset_id,
                    response=llm_response[:500]  # è®°å½•å‰ 500 ä¸ªå­—ç¬¦
                )
            except Exception as e:
                llm_gen_time = (time.perf_counter() - llm_gen_start) * 1000
                logger.error(
                    "LLM generation failed",
                    dataset_id=dataset_id,
                    error=str(e),
                    error_type=type(e).__name__,
                    llm_gen_time_ms=round(llm_gen_time, 2),
                    exc_info=True
                )
                # Use LLM to generate friendly error message
                try:
                    error_prompt = f"""ç³»ç»Ÿåœ¨å°è¯•ç”Ÿæˆ SQL æ—¶æŠ¥é”™äº†: {str(e)}
è¯·ç”¨ä¸­æ–‡ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·æŸ¥è¯¢å‡ºé”™äº†ï¼Œå¹¶å»ºè®®ä»–ä»¬æ¢ä¸€ç§é—®æ³•æˆ–æä¾›æ›´å¤šç»†èŠ‚ã€‚
ä¿æŒç®€æ´å‹å¥½ï¼Œä¸è¦æåŠæŠ€æœ¯ç»†èŠ‚ã€‚"""
                    friendly_msg = vn.submit_prompt(error_prompt)
                    execution_steps.append("LLM ç”Ÿæˆå‹å¥½é”™è¯¯æ¶ˆæ¯")
                except:
                    friendly_msg = "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç†è§£æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ã€‚èƒ½å¦è¯·æ‚¨æ¢ä¸€ç§æ–¹å¼æè¿°ï¼Œæˆ–è€…æä¾›æ›´å¤šç›¸å…³ä¿¡æ¯ï¼Ÿ"
                
                return {
                    "sql": None,
                    "columns": None,
                    "rows": None,
                    "chart_type": "clarification",
                    "answer_text": friendly_msg,
                    "steps": execution_steps
                }

            # === Step C: Intelligent Processing Loop ===
            # Process the response iteratively (max 3 rounds to prevent infinite loops)
            max_rounds = 3
            current_response = llm_response
            
            for round_num in range(1, max_rounds + 1):
                execution_steps.append(f"ç¬¬ {round_num} è½®å¤„ç†")
                logger.info(f"Round {round_num}: Processing response")
                
                # === Situation 1: Intermediate SQL ===
                intermediate_sql = VannaManager._extract_intermediate_sql(current_response)
                
                if intermediate_sql:
                    execution_steps.append(f"æ£€æµ‹åˆ°ä¸­é—´ SQL: {intermediate_sql[:100]}")
                    logger.info(
                        "Intermediate SQL detected",
                        dataset_id=dataset_id,
                        round_num=round_num,
                        intermediate_sql=intermediate_sql[:200]
                    )
                    
                    try:
                        # Execute intermediate SQL
                        intermediate_exec_start = time.perf_counter()
                        df_intermediate = pd.read_sql(intermediate_sql, engine)
                        intermediate_exec_time = (time.perf_counter() - intermediate_exec_start) * 1000
                        
                        logger.info(
                            "Intermediate SQL executed",
                            dataset_id=dataset_id,
                            row_count=len(df_intermediate),
                            exec_time_ms=round(intermediate_exec_time, 2)
                        )
                        execution_steps.append(f"ä¸­é—´ SQL æ‰§è¡ŒæˆåŠŸï¼Œè·å– {len(df_intermediate)} è¡Œ")
                        
                        # Extract distinct values
                        values = []
                        if not df_intermediate.empty:
                            values = df_intermediate.iloc[:, 0].tolist()
                            values = [str(v) for v in values if v is not None]
                        
                        logger.info(f"Intermediate values: {values}")
                        execution_steps.append(f"ä¸­é—´å€¼: {values}")
                        
                        # LLM second-round reasoning
                        reflection_prompt = f"""ç”¨æˆ·çš„é—®é¢˜æ˜¯: {question}

æˆ‘æ‰§è¡Œäº†ä¸­é—´æŸ¥è¯¢ï¼Œæ•°æ®åº“ä¸­å­˜åœ¨çš„ç›¸å…³å€¼ä¸º: {values}

è¯·æ ¹æ®è¿™äº›å€¼:
1. å¦‚æœèƒ½ç¡®å®šç”¨æˆ·æ„å›¾ï¼Œè¯·ç”Ÿæˆæœ€ç»ˆ SQL æŸ¥è¯¢(åªè¾“å‡º SQLï¼Œä¸è¦è§£é‡Š)ã€‚
2. å¦‚æœä¸èƒ½ç¡®å®š(ä¾‹å¦‚ç”¨æˆ·é—®'å¤§å®¢æˆ·'ä½†æ•°æ®åº“åªæœ‰'VIP'å’Œ'æ™®é€š')ï¼Œè¯·ç”¨**ä¸­æ–‡**ç”Ÿæˆä¸€ä¸ªæ¾„æ¸…é—®é¢˜ï¼Œè¯¢é—®ç”¨æˆ·æŒ‡çš„æ˜¯å“ªä¸€ä¸ªå€¼ã€‚

è¯·ç›´æ¥è¾“å‡º SQL æˆ–æ¾„æ¸…é—®é¢˜ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚"""
                        
                        execution_steps.append("LLM äºŒæ¬¡æ¨ç†")
                        current_response = vn.submit_prompt(reflection_prompt)
                        logger.info(f"LLM reflection response: {current_response}")
                        
                        # Continue to next iteration to process the new response
                        continue
                        
                    except Exception as e:
                        logger.warning(f"Intermediate SQL execution failed: {e}")
                        execution_steps.append(f"ä¸­é—´ SQL æ‰§è¡Œå¤±è´¥: {str(e)[:100]}")
                        # Continue with current response, might be clarification
                
                # === Situation 2: Pure Text (Clarification/Refusal) ===
                # Check if response is valid SQL
                try:
                    cleaned_sql = VannaManager._clean_sql(current_response)
                    is_sql = VannaManager._is_valid_sql(cleaned_sql)
                except:
                    is_sql = False
                    cleaned_sql = ""
                
                if not is_sql:
                    # It's pure text - treat as clarification
                    execution_steps.append("LLM è¿”å›çº¯æ–‡æœ¬(æ¾„æ¸…/è¯´æ˜)")
                    logger.info(f"Pure text response: {current_response}")
                    
                    return {
                        "sql": None,
                        "columns": None,
                        "rows": None,
                        "chart_type": "clarification",
                        "answer_text": current_response,
                        "steps": execution_steps
                    }
                
                # === Situation 3: Valid SQL ===
                execution_steps.append("æ£€æµ‹åˆ°æœ‰æ•ˆ SQL")
                
                # æ·»åŠ LIMITå­å¥é˜²æ­¢æŸ¥è¯¢è¿‡å¤šæ•°æ®å¯¼è‡´è¶…æ—¶
                # åˆ¤æ–­ SQL æ˜¯å¦å·²æœ‰ LIMIT å­å¥
                sql_upper = cleaned_sql.upper()
                has_limit = 'LIMIT' in sql_upper
                
                if not has_limit:
                    # åœ¨SQLæœ«å°¾æ·»åŠ LIMIT
                    cleaned_sql = cleaned_sql.rstrip(';').strip() + ' LIMIT 1000'
                    execution_steps.append("è‡ªåŠ¨æ·»åŠ  LIMIT 1000 é˜²æ­¢è¿‡å¤šæ•°æ®")
                elif has_limit:
                    # æ£€æŸ¥ LIMIT å€¼æ˜¯å¦è¿‡å¤§ï¼Œå¦‚æœè¶…è¿‡ 5000 åˆ™æ›¿æ¢ä¸º 1000
                    import re
                    limit_match = re.search(r'LIMIT\s+(\d+)', sql_upper)
                    if limit_match:
                        limit_value = int(limit_match.group(1))
                        if limit_value > 5000:
                            # æ›¿æ¢ä¸ºæ›´å°çš„å€¼
                            cleaned_sql = re.sub(r'LIMIT\s+\d+', 'LIMIT 1000', cleaned_sql, flags=re.IGNORECASE)
                            execution_steps.append(f"å°† LIMIT {limit_value} è°ƒæ•´ä¸º LIMIT 1000 é˜²æ­¢è¶…æ—¶")
                
                try:
                    # Execute the SQL with timeout protection
                    final_exec_start = time.perf_counter()
                    df = pd.read_sql(cleaned_sql, engine)
                    final_exec_time = (time.perf_counter() - final_exec_start) * 1000
                    
                    logger.info(
                        "SQL executed successfully",
                        dataset_id=dataset_id,
                        sql=cleaned_sql[:200],  # æˆªæ–­ SQL
                        row_count=len(df),
                        column_count=len(df.columns),
                        sql_exec_time_ms=round(final_exec_time, 2)
                    )
                    execution_steps.append(f"SQL æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(df)} è¡Œ")
                    
                    # Chart Type Inference
                    chart_type = VannaManager._infer_chart_type(df)
                    execution_steps.append(f"æ¨æ–­å›¾è¡¨ç±»å‹: {chart_type}")
                    
                    # Serialize data
                    cleaned_rows = VannaManager._serialize_dataframe(df)
                    
                    # Generate Business Insight (Analyst Agent)
                    insight = None
                    if len(df) > 0:  # åªæœ‰æ•°æ®ä¸ä¸ºç©ºæ—¶æ‰ç”Ÿæˆåˆ†æ
                        try:
                            execution_steps.append("æ­£åœ¨ç”Ÿæˆä¸šåŠ¡åˆ†æ...")
                            insight = VannaManager.generate_data_insight(
                                question=question,
                                sql=cleaned_sql,
                                df=df,
                                dataset_id=dataset_id
                            )
                            execution_steps.append("ä¸šåŠ¡åˆ†æç”Ÿæˆå®Œæˆ")
                            logger.info(f"Business insight generated: {insight[:50]}...")
                        except Exception as insight_error:
                            logger.warning(f"Failed to generate insight: {insight_error}")
                            execution_steps.append("ä¸šåŠ¡åˆ†æç”Ÿæˆå¤±è´¥")
                            insight = None
                    
                    # Prepare result
                    result = {
                        "sql": cleaned_sql,
                        "columns": df.columns.tolist(),
                        "rows": cleaned_rows,
                        "chart_type": chart_type,
                        "summary": None,
                        "steps": execution_steps,
                        "from_cache": False,
                        "insight": insight  # æ–°å¢åˆ†æå¸ˆ Agent çš„è¾“å‡º
                    }
                    
                    # === Step D: Write to Cache ===
                    # å†™å…¥ SQL ç¼“å­˜ï¼Œ24å°æ—¶ TTL
                    if use_cache:
                        try:
                            sql_cache_key = generate_cache_key("bi:sql_cache", dataset_id, question)
                            # 24å°æ—¶ = 86400 ç§’
                            await redis_service.set(sql_cache_key, cleaned_sql, expire=86400)
                            logger.info(
                                "SQL cached",
                                dataset_id=dataset_id,
                                cache_key=sql_cache_key[:50],
                                ttl_hours=24
                            )
                            execution_steps.append("SQLå·²ç¼“å­˜ (TTL: 24h)")
                        except Exception as e:
                            logger.warning(
                                "ç¼“å­˜å†™å…¥å¤±è´¥",
                                dataset_id=dataset_id,
                                error=str(e)
                            )
                            execution_steps.append(f"SQLç¼“å­˜å†™å…¥å¤±è´¥: {str(e)[:50]}")
                    
                    # è®°å½•æ€»è€—æ—¶
                    total_time = (time.perf_counter() - start_time) * 1000
                    logger.info(
                        "Request completed",
                        dataset_id=dataset_id,
                        total_time_ms=round(total_time, 2),
                        from_cache=False
                    )
                    
                    return result
                    
                except Exception as e:
                    error_msg = str(e)
                    logger.error(
                        "SQL execution failed",
                        dataset_id=dataset_id,
                        sql=cleaned_sql[:200],
                        error=error_msg[:200],
                        error_type=type(e).__name__,
                        exc_info=True
                    )
                    execution_steps.append(f"SQL æ‰§è¡Œå¤±è´¥: {error_msg[:100]}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥è¶…æ—¶é”™è¯¯
                    is_timeout_error = '2013' in error_msg or 'Lost connection' in error_msg or 'timeout' in error_msg.lower()
                    
                    # å¦‚æœæ˜¯è¶…æ—¶é”™è¯¯ï¼Œç›´æ¥è¿”å›å‹å¥½æç¤ºï¼Œä¸å†é‡è¯•
                    if is_timeout_error:
                        error_prompt = f"""æŸ¥è¯¢æ‰§è¡Œè¶…æ—¶äº†ã€‚ç”¨æˆ·çš„é—®é¢˜æ˜¯: {question}

è¯·ç”¨ä¸­æ–‡ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·ï¼š
1. æŸ¥è¯¢è€—æ—¶è¿‡é•¿ï¼Œå¯èƒ½æ˜¯æ•°æ®é‡è¾ƒå¤§
2. å»ºè®®ç¼©å°æ—¶é—´èŒƒå›´ï¼Œæ¯”å¦‚æ”¹ä¸ºâ€œæœ€è¿‘ 7 å¤©â€æˆ–â€œæœ¬å‘¨â€
3. æˆ–è€…æ·»åŠ æ›´å…·ä½“çš„ç­›é€‰æ¡ä»¶

ä¿æŒç®€æ´å‹å¥½ï¼Œä¸è¦æåŠæŠ€æœ¯ç»†èŠ‚ã€‚"""
                        try:
                            friendly_msg = vn.submit_prompt(error_prompt)
                            execution_steps.append("æ£€æµ‹åˆ°è¶…æ—¶ï¼Œç”Ÿæˆä¼˜åŒ–å»ºè®®")
                        except:
                            friendly_msg = "æŠ±æ­‰ï¼ŒæŸ¥è¯¢è€—æ—¶è¿‡é•¿ã€‚å»ºè®®æ‚¨ç¼©å°æ—¶é—´èŒƒå›´ï¼ˆæ¯”å¦‚æ”¹ä¸ºâ€˜æœ€è¿‘ 7 å¤©â€™æˆ–â€˜æœ¬å‘¨â€™ï¼‰ï¼Œæˆ–è€…æ·»åŠ æ›´å…·ä½“çš„ç­›é€‰æ¡ä»¶ã€‚"
                        
                        return {
                            "sql": cleaned_sql,
                            "columns": None,
                            "rows": None,
                            "chart_type": "clarification",
                            "answer_text": friendly_msg,
                            "steps": execution_steps
                        }
                    
                    # If not the last round, try SQL correction with LLM
                    if round_num < max_rounds:
                        try:
                            execution_steps.append("å°è¯• LLM ä¿®æ­£ SQL")
                            
                            db_type = dataset.datasource.type.upper()
                            correction_prompt = f"""ä»¥ä¸‹ SQL åœ¨ {db_type} æ•°æ®åº“ä¸Šæ‰§è¡Œå¤±è´¥:

SQL:
{cleaned_sql}

é”™è¯¯:
{error_msg}

è¯·åˆ†æå¹¶ä¿®æ­£è¿™ä¸ª SQLï¼Œä½¿å…¶èƒ½æ­£ç¡®æ‰§è¡Œã€‚å¦‚æœæ— æ³•ä¿®æ­£ï¼Œè¯·ç”¨ä¸­æ–‡è¯´æ˜åŸå› ã€‚
åªè¾“å‡ºä¿®æ­£åçš„ SQL æˆ–è¯´æ˜ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚"""
                            
                            current_response = vn.submit_prompt(correction_prompt)
                            logger.info(f"LLM correction response: {current_response}")
                            execution_steps.append("LLM å·²ç”Ÿæˆä¿®æ­£æ–¹æ¡ˆ")
                            
                            # Continue to next iteration
                            continue
                            
                        except Exception as correction_error:
                            logger.error(f"SQL correction failed: {correction_error}")
                            execution_steps.append(f"ä¿®æ­£è¿‡ç¨‹å‡ºé”™: {str(correction_error)[:100]}")
                    
                    # Last round or correction failed - use LLM for friendly error
                    try:
                        error_prompt = f"""SQL æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {error_msg}
ç”¨æˆ·çš„é—®é¢˜æ˜¯: {question}

è¯·ç”¨ä¸­æ–‡ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·æŸ¥è¯¢å‡ºé”™äº†ï¼Œå¹¶å»ºè®®ä»–ä»¬:
1. æ£€æŸ¥é—®é¢˜æè¿°æ˜¯å¦æ¸…æ™°
2. æˆ–è€…æ¢ä¸€ç§é—®æ³•
3. æˆ–è€…æä¾›æ›´å¤šèƒŒæ™¯ä¿¡æ¯

ä¿æŒç®€æ´å‹å¥½ï¼Œä¸è¦æåŠæŠ€æœ¯ç»†èŠ‚ã€‚"""
                        friendly_msg = vn.submit_prompt(error_prompt)
                        execution_steps.append("LLM ç”Ÿæˆå‹å¥½é”™è¯¯æ¶ˆæ¯")
                    except:
                        friendly_msg = f"æŠ±æ­‰ï¼ŒæŸ¥è¯¢æ‰§è¡Œé‡åˆ°äº†é—®é¢˜ã€‚å»ºè®®æ‚¨æ¢ä¸€ç§æ–¹å¼æè¿°é—®é¢˜ï¼Œæˆ–è€…æä¾›æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚"
                    
                    return {
                        "sql": cleaned_sql,
                        "columns": None,
                        "rows": None,
                        "chart_type": "clarification",
                        "answer_text": friendly_msg,
                        "steps": execution_steps
                    }
            
            # Reached max rounds without success
            execution_steps.append(f"è¾¾åˆ°æœ€å¤§è½®æ¬¡ ({max_rounds})")
            return {
                "sql": None,
                "columns": None,
                "rows": None,
                "chart_type": "clarification",
                "answer_text": "æŠ±æ­‰ï¼Œç»è¿‡å¤šè½®åˆ†æåä»æ— æ³•å‡†ç¡®ç†è§£æ‚¨çš„é—®é¢˜ã€‚èƒ½å¦è¯·æ‚¨æä¾›æ›´è¯¦ç»†çš„æè¿°æˆ–æ¢ä¸€ä¸ªé—®æ³•ï¼Ÿ",
                "steps": execution_steps
            }
        
        except Exception as e:
            # === Situation 4: Global Exception (Fallback) ===
            logger.error(f"Unexpected error in generate_result: {e}", exc_info=True)
            execution_steps.append(f"å…¨å±€å¼‚å¸¸: {str(e)[:100]}")
            
            # Try to use LLM for friendly error message
            try:
                vn = VannaManager.get_legacy_vanna(dataset_id)
                error_prompt = f"""ç³»ç»ŸæŠ¥é”™äº†: {str(e)}
è¯·ç”¨ä¸­æ–‡ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·æŸ¥è¯¢å‡ºé”™äº†ï¼Œå¹¶å»ºè®®ä»–ä»¬æ¢ä¸€ç§é—®æ³•ã€‚
ä¿æŒç®€æ´å‹å¥½ï¼Œä¸è¦æåŠæŠ€æœ¯ç»†èŠ‚ã€‚"""
                friendly_msg = vn.submit_prompt(error_prompt)
                execution_steps.append("LLM ç”Ÿæˆå¼‚å¸¸å‹å¥½æ¶ˆæ¯")
            except:
                friendly_msg = "æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†æ„å¤–é”™è¯¯ã€‚è¯·ç¨åé‡è¯•æˆ–æ¢ä¸€ç§æ–¹å¼æè¿°æ‚¨çš„é—®é¢˜ã€‚"
            
            return {
                "sql": None,
                "columns": None,
                "rows": None,
                "chart_type": "clarification",
                "answer_text": friendly_msg,
                "steps": execution_steps
            }

    @staticmethod
    def _infer_chart_type(df: pd.DataFrame) -> str:
        """
        Infer the most suitable chart type based on DataFrame structure.
        
        Args:
            df: DataFrame to analyze
            
        Returns:
            str: Chart type ('line', 'bar', 'pie', or 'table')
        """
        if df is None or df.empty:
            return "table"
        
        # Get column types
        lower_cols = [str(c).lower() for c in df.columns]
        has_date = any(x in c for c in lower_cols for x in ['date', 'time', 'year', 'month', 'day'])
        
        # Check data types
        dtypes = df.dtypes.values
        has_num = any(pd.api.types.is_numeric_dtype(t) for t in dtypes)
        has_str = any(pd.api.types.is_string_dtype(t) or pd.api.types.is_object_dtype(t) or pd.api.types.is_datetime64_any_dtype(t) for t in dtypes)
        
        # Decision logic
        if len(df.columns) == 2 and has_date:
            return "line"
        elif len(df.columns) == 2 and has_num and has_str:
            # Could be pie chart if reasonable number of categories
            if len(df) <= 10:
                return "pie"
            return "bar"
        elif len(df.columns) >= 2 and has_num and has_str:
            return "bar"
        else:
            return "table"
    
    @staticmethod
    def _serialize_dataframe(df: pd.DataFrame) -> list:
        """
        Serialize DataFrame to JSON-compatible format.
        
        Args:
            df: DataFrame to serialize
            
        Returns:
            list: List of dictionaries with serialized values
        """
        def serialize(obj):
            if isinstance(obj, (date, datetime)):
                return obj.isoformat()
            if isinstance(obj, Decimal):
                return float(obj)
            return obj
        
        rows = df.to_dict(orient='records')
        cleaned_rows = []
        for row in rows:
            new_row = {k: serialize(v) for k, v in row.items()}
            cleaned_rows.append(new_row)
        
        return cleaned_rows

    @staticmethod
    def _remove_intermediate_marker(response: str) -> str:
        """
        Remove 'intermediate_sql' marker from LLM response.
        
        Args:
            response: Raw LLM response that may contain 'intermediate_sql' marker
            
        Returns:
            str: Cleaned response with marker removed
        """
        if not response or not isinstance(response, str):
            return response
        
        # Remove 'intermediate_sql' marker (case insensitive)
        # Pattern: intermediate_sql\n or intermediate sql\n or intermediate_sql:
        cleaned = re.sub(r'intermediate[_\s]?sql[:\s]*\n?', '', response, flags=re.IGNORECASE)
        
        return cleaned.strip()

    @staticmethod
    def _is_valid_sql(sql: str) -> bool:
        """
        Check if the string is a valid SQL query.
        
        Args:
            sql: String to validate
            
        Returns:
            bool: True if valid SQL, False otherwise
        """
        if not sql or not isinstance(sql, str):
            return False
        
        sql_upper = sql.upper().strip()
        
        # Must contain SELECT and FROM
        # Simple validation - just check for basic SQL structure
        has_select = 'SELECT' in sql_upper
        has_from = 'FROM' in sql_upper
        
        # Check it's not just text with these words
        # Valid SQL should have SELECT near the beginning
        if has_select and has_from:
            select_pos = sql_upper.find('SELECT')
            # SELECT should be within first 50 characters (allowing for comments/whitespace)
            if select_pos < 50:
                return True
        
        return False

    @staticmethod
    def _ensure_clean_sql(sql: str) -> str:
        """
        Ensure SQL is completely clean before execution.
        Removes all markdown, intermediate markers, and extra whitespace.
        
        Args:
            sql: SQL string to clean
            
        Returns:
            str: Cleaned SQL ready for execution
        """
        if not sql or not isinstance(sql, str):
            return sql
        
        # Remove markdown code blocks
        sql = re.sub(r'```sql\s*', '', sql, flags=re.IGNORECASE)
        sql = re.sub(r'```\s*', '', sql)
        
        # Remove intermediate_sql marker if somehow still present
        sql = re.sub(r'intermediate[_\s]?sql[:\s]*', '', sql, flags=re.IGNORECASE)
        
        # Remove leading/trailing whitespace
        sql = sql.strip()
        
        # Remove multiple spaces
        sql = re.sub(r'\s+', ' ', sql)
        
        return sql

    @staticmethod
    def _extract_intermediate_sql(response: str) -> str:
        """
        Extract intermediate SQL from LLM response.
        
        Vanna 2.0 may return intermediate SQL when it needs more context.
        This detects patterns like:
        - Response contains 'intermediate' keyword
        - Response contains SQL but also contains questions or uncertainties
        - Response starts with SELECT but contains reasoning text
        
        Returns:
            str: The intermediate SQL if detected, empty string otherwise
        """
        if not response or not isinstance(response, str):
            return ""
        
        response_lower = response.lower()
        
        # Pattern 1: Explicit intermediate_sql marker
        if 'intermediate_sql' in response_lower or 'intermediate sql' in response_lower:
            # Try to extract SQL after the marker
            sql_start = response.find('SELECT')
            if sql_start == -1:
                sql_start = response.find('select')
            
            if sql_start >= 0:
                # Extract until we hit a double newline or end of string
                sql_end = response.find('\n\n', sql_start)
                if sql_end > 0:
                    intermediate_sql = response[sql_start:sql_end].strip()
                else:
                    intermediate_sql = response[sql_start:].strip()
                
                # Clean up
                intermediate_sql = VannaManager._clean_sql(intermediate_sql)
                if 'SELECT' in intermediate_sql.upper():
                    return intermediate_sql
        
        # Pattern 2: Response contains questions/uncertainties AND SQL
        uncertainty_keywords = ['ä¸ç¡®å®š', 'ä¸çŸ¥é“', 'uncertain', 'unclear', 'æ˜¯æŒ‡', 'æ˜¯ä¸æ˜¯', 'å¯èƒ½', 'might be', 'could be', 'éœ€è¦æ¾„æ¸…']
        has_uncertainty = any(keyword in response_lower for keyword in uncertainty_keywords)
        
        if has_uncertainty and 'select' in response_lower:
            # Extract the SQL part
            sql_start = response.find('SELECT')
            if sql_start == -1:
                sql_start = response.find('select')
            
            if sql_start >= 0:
                # Look for DISTINCT type queries (common intermediate pattern)
                if 'distinct' in response_lower[sql_start:sql_start+200]:
                    sql_end = response.find('\n\n', sql_start)
                    if sql_end > 0:
                        intermediate_sql = response[sql_start:sql_end].strip()
                    else:
                        # Try to find end of SQL statement
                        sql_end = response.find(';', sql_start)
                        if sql_end > 0:
                            intermediate_sql = response[sql_start:sql_end+1].strip()
                        else:
                            intermediate_sql = response[sql_start:].strip()
                    
                    intermediate_sql = VannaManager._clean_sql(intermediate_sql)
                    if 'SELECT' in intermediate_sql.upper() and 'DISTINCT' in intermediate_sql.upper():
                        return intermediate_sql
        
        return ""

    @staticmethod
    def _is_clarification_request(response: str) -> bool:
        """
        Check if LLM response is asking for clarification rather than providing SQL.
        
        Returns:
            bool: True if response appears to be a clarification request
        """
        if not response or not isinstance(response, str):
            return False
        
        response_lower = response.lower()
        
        # Keywords indicating clarification needed
        clarification_patterns = [
            'æ— æ³•ç¡®å®š', 'ä¸ç¡®å®š', 'éœ€è¦æ›´å¤šä¿¡æ¯', 'è¯·æ˜ç¡®', 'è¯·æŒ‡å®š',
            'cannot determine', 'unclear', 'need more information', 
            'please clarify', 'please specify', 'could you clarify',
            'è¯·é—®', 'æ˜¯æŒ‡', 'æ˜¯ä¸æ˜¯', 'çš„æ„æ€',
            'do you mean', 'what do you mean', 'which',
            'æ²¡æœ‰æ‰¾åˆ°', 'not found', 'cannot find'
        ]
        
        has_clarification = any(pattern in response_lower for pattern in clarification_patterns)
        
        # Also check if response doesn't contain valid SQL
        has_sql = 'select' in response_lower and 'from' in response_lower
        
        # If it has clarification keywords and no SQL, it's likely a clarification request
        # OR if it has clarification keywords and very short SQL (< 50 chars), might be incomplete
        if has_clarification:
            if not has_sql:
                return True
            else:
                # Extract potential SQL and check if it's complete
                sql_part = response[response_lower.find('select'):]
                if len(sql_part.strip()) < 50:  # Too short to be a real query
                    return True
        
        return False

    @staticmethod
    def _clean_sql(sql: str) -> str:
        """
        Clean and normalize SQL string from AI output.
        Removes markdown code blocks and extra whitespace.
        """
        if not sql or not isinstance(sql, str):
            raise ValueError("Invalid SQL output")
        
        sql = sql.strip()
        
        # Remove markdown code blocks
        if sql.startswith("```"):
            sql = sql.split("\n", 1)[1] if "\n" in sql else sql
            if sql.endswith("```"):
                sql = sql.rsplit("\n", 1)[0]
        
        sql = sql.replace("```sql", "").replace("```", "").strip()
        
        return sql
    
    @staticmethod
    def generate_summary(question: str, df: pd.DataFrame, dataset_id: int) -> str:
        """
        Generate AI-powered business summary of query results.
        
        Args:
            question: User's original question
            df: Query result DataFrame
            dataset_id: Dataset ID for Vanna instance
            
        Returns:
            str: AI-generated summary text (100 words max, in Chinese)
        """
        if df is None or df.empty:
            return "æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"
        
        try:
            # Get Vanna instance for LLM
            vn = VannaManager.get_legacy_vanna(dataset_id)
            
            # 1. Prepare data preview
            # Convert head (max 5 rows) to markdown table
            data_preview_parts = []
            
            # Add head data
            head_df = df.head(5)
            data_preview_parts.append("å‰ 5 è¡Œæ•°æ®ï¼š")
            data_preview_parts.append(head_df.to_markdown(index=False))
            
            # Add statistical description if contains numeric columns
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) > 0:
                describe_df = df[numeric_cols].describe()
                data_preview_parts.append("\nç»Ÿè®¡æè¿°ï¼š")
                data_preview_parts.append(describe_df.to_markdown())
            
            data_preview = "\n".join(data_preview_parts)
            
            # 2. Construct prompt
            prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{question}

æŸ¥è¯¢ç»“æœæ•°æ®å¦‚ä¸‹ï¼š
{data_preview}

è¯·åŸºäºä¸Šè¿°æ•°æ®ï¼Œç”¨ä¸€æ®µç®€ç»ƒçš„ä¸­æ–‡æ€»ç»“æ•°æ®è¶‹åŠ¿æˆ–å…³é”®å‘ç°ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚

è¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºæ€»ç»“æ–‡å­—ï¼Œä¸è¦æ·»åŠ â€œæ€»ç»“ï¼šâ€ç­‰å‰ç¼€
2. ç”¨ä¸šåŠ¡è¯­è¨€ï¼Œé¿å…æŠ€æœ¯è¯æ±‡
3. çªå‡ºå…³é”®æ•°å­—å’Œè¶‹åŠ¿
4. ä¿æŒç®€æ´ã€å‹å¥½"""
            
            # 3. Call LLM
            summary = vn.submit_prompt(prompt)
            
            # 4. Clean and validate
            summary = summary.strip()
            
            # Remove common prefixes
            prefixes_to_remove = ["æ€»ç»“ï¼š", "åˆ†æï¼š", "AIæ€»ç»“ï¼š", "æ•°æ®åˆ†æï¼š"]
            for prefix in prefixes_to_remove:
                if summary.startswith(prefix):
                    summary = summary[len(prefix):].strip()
            
            # Limit length (just in case LLM ignores the limit)
            if len(summary) > 200:
                summary = summary[:200] + "..."
            
            return summary
            
        except Exception as e:
            logger.error(f"Failed to generate summary: {e}")
            return "æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    @staticmethod
    def generate_data_insight(question: str, sql: str, df: pd.DataFrame, dataset_id: int) -> str:
        """
        Generate AI-powered business insight as an analyst agent.
        Compresses data into a summary before sending to LLM to reduce token usage.
        
        Args:
            question: User's original question
            sql: SQL query that generated the data
            df: Query result DataFrame
            dataset_id: Dataset ID for Vanna instance
            
        Returns:
            str: AI-generated business insight (Markdown format, in Chinese)
        """
        if df is None or df.empty:
            return "æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆä¸šåŠ¡åˆ†æã€‚"
        
        try:
            # Get Vanna instance for LLM
            vn = VannaManager.get_legacy_vanna(dataset_id)
            
            # === æ•°æ®å‹ç¼©ç­–ç•¥ ===
            data_summary_parts = []
            
            # 1. å‰ 5 è¡Œæ•°æ®ï¼ˆä½¿ç”¨ markdown è¡¨æ ¼æ ¼å¼ï¼‰
            head_df = df.head(5)
            data_summary_parts.append("### æ•°æ®æ ·æœ¬ï¼ˆå‰ 5 è¡Œï¼‰")
            data_summary_parts.append(head_df.to_markdown(index=False))
            
            # 2. æ•°å€¼åˆ—çš„ç»Ÿè®¡æè¿°
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) > 0:
                describe_df = df[numeric_cols].describe()
                data_summary_parts.append("\n### ç»Ÿè®¡æè¿°ï¼ˆæ•°å€¼åˆ—ï¼‰")
                data_summary_parts.append(describe_df.to_markdown())
            
            # 3. æ•°æ®å…ƒä¿¡æ¯
            data_summary_parts.append(f"\n### æ•°æ®å…ƒä¿¡æ¯")
            data_summary_parts.append(f"- åˆ—åï¼š{', '.join(df.columns.tolist())}")
            data_summary_parts.append(f"- æ•°æ®æ€»é‡ï¼š{len(df)} è¡Œ")
            
            data_summary = "\n".join(data_summary_parts)
            
            # === Prompt è®¾è®¡ï¼šæ‰®æ¼”é«˜çº§å•†ä¸šåˆ†æå¸ˆ ===
            system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å•†ä¸šæ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ•°æ®ä¸­æŒ–æ˜æ´å¯Ÿå’Œè¶‹åŠ¿ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç”¨æˆ·çš„é—®é¢˜ã€SQL æŸ¥è¯¢é€»è¾‘å’Œæ•°æ®æ‘˜è¦ï¼Œæä¾›ç®€æ´çš„ä¸šåŠ¡åˆ†æã€‚

åˆ†æè¦æ±‚ï¼š
1. ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡º
2. é‡ç‚¹å…³æ³¨æ•°æ®è¶‹åŠ¿ã€å¼‚å¸¸å€¼ã€å…³é”®å‘ç°
3. ç”¨ä¸šåŠ¡è¯­è¨€ï¼Œé¿å…æŠ€æœ¯æœ¯è¯­
4. ç¯‡å¹…æ§åˆ¶åœ¨ 150 å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºåˆ†æå†…å®¹ï¼Œä¸è¦æ·»åŠ "åˆ†æï¼š"ç­‰å‰ç¼€
6. ä½¿ç”¨ä¸­æ–‡"""
            
            user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š
{question}

SQL æŸ¥è¯¢é€»è¾‘ï¼š
```sql
{sql}
```

{data_summary}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæä¾›ä¸šåŠ¡æ´å¯Ÿåˆ†æï¼š"""
            
            # === è°ƒç”¨ LLM ===
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            insight = vn.submit_prompt(messages)
            
            # === æ¸…ç†å’ŒéªŒè¯ ===
            insight = insight.strip()
            
            # ç§»é™¤å¸¸è§å‰ç¼€
            prefixes_to_remove = ["åˆ†æï¼š", "ä¸šåŠ¡æ´å¯Ÿï¼š", "AI åˆ†æï¼š", "æ•°æ®æ´å¯Ÿï¼š", "æ€»ç»“ï¼š"]
            for prefix in prefixes_to_remove:
                if insight.startswith(prefix):
                    insight = insight[len(prefix):].strip()
            
            # é•¿åº¦é™åˆ¶ï¼ˆé˜²æ­¢ LLM å¿½ç•¥æŒ‡ä»¤ï¼‰
            if len(insight) > 300:
                insight = insight[:300] + "..."
            
            logger.info(f"Generated insight for dataset {dataset_id}: {insight[:50]}...")
            return insight
            
        except Exception as e:
            logger.error(f"Failed to generate data insight: {e}")
            return "ä¸šåŠ¡åˆ†æç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    @staticmethod
    def analyze_relationships(dataset_id: int, table_names: list[str], db_session: Session) -> dict:
        """
        Use LLM to analyze potential relationships between tables.
        
        Args:
            dataset_id: Dataset ID
            table_names: List of table names to analyze
            db_session: Database session
            
        Returns:
            dict: {
                'edges': [{'source': 'users', 'target': 'orders', 'source_col': 'id', 'target_col': 'user_id', 'type': 'left'}],
                'nodes': [{'table_name': 'users', 'fields': [...]}]
            }
        """
        try:
            # Get dataset and datasource
            stmt = select(Dataset).options(selectinload(Dataset.datasource)).where(Dataset.id == dataset_id)
            result = db_session.execute(stmt)
            dataset = result.scalars().first()
            
            if not dataset or not dataset.datasource:
                raise ValueError("æ•°æ®é›†æˆ–æ•°æ®æºä¸å­˜åœ¨")
            
            datasource = dataset.datasource
            
            # 1. Get DDLs for all tables
            ddl_list = []
            nodes = []
            
            # Get engine and inspector once
            engine = DBInspector.get_engine(datasource)
            inspector = inspect(engine)
            
            for table_name in table_names:
                try:
                    # Get table columns using inspector (faster than DDL generation)
                    columns = inspector.get_columns(table_name)
                    
                    # Build simple DDL representation
                    ddl_parts = [f"CREATE TABLE {table_name} ("]
                    col_definitions = []
                    for col in columns:
                        col_def = f"  {col['name']} {str(col['type'])}"
                        if not col.get('nullable', True):
                            col_def += " NOT NULL"
                        col_definitions.append(col_def)
                    ddl_parts.append(",\n".join(col_definitions))
                    ddl_parts.append(")")
                    ddl = "\n".join(ddl_parts)
                    
                    ddl_list.append(f"-- {table_name}\n{ddl}")
                    
                    # Build fields for nodes
                    fields = []
                    for col in columns:
                        fields.append({
                            'name': col['name'],
                            'type': str(col['type']),
                            'nullable': col.get('nullable', True)
                        })
                    
                    nodes.append({
                        'table_name': table_name,
                        'fields': fields
                    })
                    
                except Exception as e:
                    logger.warning(f"Failed to get structure for {table_name}: {e}")
                    # Try to continue with other tables
                    continue
            
            if not ddl_list:
                raise ValueError("æ— æ³•è·å–è¡¨ç»“æ„ä¿¡æ¯")
            
            # 2. Construct prompt for LLM
            ddl_content = "\n\n".join(ddl_list)
            
            # Optimized prompt for strict JSON output
            prompt = f"""Analyze the following database table structures and identify potential foreign key relationships:

{ddl_content}

Analysis rules:
1. Look for naming patterns (e.g., user_id likely references users.id)
2. Consider common relationship patterns (order_id, product_id, customer_id, etc.)
3. Verify field type compatibility
4. Consider business logic reasonability

**CRITICAL**: Strictly return ONLY a JSON array. Do not use Markdown formatting. No explanation. No code blocks.

Expected format:
[{{
  "source": "table_a",
  "target": "table_b",
  "source_col": "id",
  "target_col": "a_id",
  "type": "left",
  "confidence": "high"
}}]

If no relationships found, return: []

Your response (JSON array only):"""
            
            # 3. Call LLM directly using OpenAI client to avoid ChromaDB conflicts
            # Using direct API call instead of Vanna instance
            client = OpenAIClient(
                api_key=settings.DASHSCOPE_API_KEY,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            
            response = client.chat.completions.create(
                model=settings.QWEN_MODEL,
                messages=[
                    {"role": "system", "content": "You are a database relationship analyzer. You analyze table structures and return JSON arrays."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1  # Lower temperature for more structured output
            )
            
            llm_response = response.choices[0].message.content
            
            logger.info(f"LLM relationship analysis response: {llm_response}")
            
            # 4. Enhanced JSON parsing with robust cleaning
            try:
                # Step 1: Clean markdown code blocks
                cleaned_response = llm_response.strip()
                
                # Remove markdown code block markers (```json, ```, etc.)
                cleaned_response = re.sub(r'^```(?:json)?\s*', '', cleaned_response, flags=re.IGNORECASE)
                cleaned_response = re.sub(r'\s*```$', '', cleaned_response)
                cleaned_response = cleaned_response.strip()
                
                # Step 2: Try direct JSON parsing
                try:
                    edges = json.loads(cleaned_response)
                    logger.info("JSON parsed successfully on first attempt")
                except json.JSONDecodeError as e:
                    logger.warning(f"First JSON parse failed: {e}. Attempting regex extraction...")
                    
                    # Step 3: Try extracting JSON array using regex
                    match = re.search(r'\[.*\]', cleaned_response, re.DOTALL)
                    if match:
                        json_str = match.group()
                        edges = json.loads(json_str)
                        logger.info("JSON extracted and parsed successfully using regex")
                    else:
                        # Step 4: Last resort - try to find any valid JSON structure
                        # Look for content between first [ and last ]
                        start_idx = cleaned_response.find('[')
                        end_idx = cleaned_response.rfind(']')
                        
                        if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                            json_str = cleaned_response[start_idx:end_idx + 1]
                            edges = json.loads(json_str)
                            logger.info("JSON parsed successfully using index-based extraction")
                        else:
                            # No valid JSON found, return empty
                            logger.error(f"Could not find valid JSON array in response: {cleaned_response[:200]}")
                            edges = []
                
                # Validate structure
                if not isinstance(edges, list):
                    logger.warning(f"LLM response is not a list, wrapping in list: {type(edges)}")
                    edges = [edges] if isinstance(edges, dict) else []
                
                # Filter and validate edges
                valid_edges = []
                for edge in edges:
                    if isinstance(edge, dict) and all(k in edge for k in ['source', 'target', 'source_col', 'target_col']):
                        # Ensure type is set
                        if 'type' not in edge:
                            edge['type'] = 'left'
                        # Ensure confidence is set
                        if 'confidence' not in edge:
                            edge['confidence'] = 'medium'
                        valid_edges.append(edge)
                    else:
                        logger.warning(f"Skipping invalid edge structure: {edge}")
                
                logger.info(f"Analyzed {len(valid_edges)} valid relationships between {len(table_names)} tables")
                
                return {
                    'edges': valid_edges,
                    'nodes': nodes
                }
                
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse LLM JSON response after all attempts: {e}")
                logger.error(f"Raw response: {llm_response}")
                logger.error(f"Cleaned response: {cleaned_response[:500]}")
                # Return empty edges if parsing fails
                return {
                    'edges': [],
                    'nodes': nodes
                }
            except Exception as e:
                logger.error(f"Unexpected error during JSON parsing: {e}")
                return {
                    'edges': [],
                    'nodes': nodes
                }
        
        except Exception as e:
            logger.error(f"Failed to analyze relationships: {e}")
            raise ValueError(f"å…³è”åˆ†æå¤±è´¥: {str(e)}")
