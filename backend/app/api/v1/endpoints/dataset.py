from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, UploadFile, File, Form
from sqlalchemy.orm import Session
from sqlalchemy import text
from typing import List, Optional
from datetime import datetime
import time

from app.db.session import get_db, SessionLocal
from app.api.deps import get_current_user, apply_ownership_filter
from app.models.metadata import Dataset, DataSource, BusinessTerm, User, TrainingLog, ComputedMetric
from app.schemas.dataset import (
    DatasetCreate, DatasetResponse, DatasetUpdateTables,
    BusinessTermCreate, BusinessTermResponse,
    AnalyzeRelationshipsRequest, AnalyzeRelationshipsResponse,
    EdgeResponse, NodeResponse, FieldResponse,
    CreateViewRequest, TrainingLogResponse, TrainingDataResponse,
    TrainQARequest, TrainDocRequest, SuggestedQuestions
)
from app.services.vanna import (
    VannaInstanceManager,
    VannaTrainingService,
    VannaCacheService,
    VannaTrainingDataService,
    VannaAnalystService
)
from app.services.vanna.facade import VannaManager
from app.services.vanna.relationship_analyzer import RelationshipAnalyzer
from app.services.duckdb_service import DuckDBService

router = APIRouter()

def run_training_task(dataset_id: int, table_names: list[str]):
    """
    Background task wrapper to ensure a separate DB session.
    """
    db = SessionLocal()
    try:
        VannaTrainingService.train_dataset(dataset_id, table_names, db)
    finally:
        db.close()

@router.post("/", response_model=DatasetResponse)
def create_dataset(
    dataset_in: DatasetCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Create a new dataset.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šéœ€è¦éªŒè¯ DataSource çš„æ‰€æœ‰æƒ
    """
    # éªŒè¯ DataSource è®¿é—®æƒé™
    ds_query = db.query(DataSource).filter(DataSource.id == dataset_in.datasource_id)
    ds_query = apply_ownership_filter(ds_query, DataSource, current_user)
    datasource = ds_query.first()
    
    if not datasource:
        raise HTTPException(status_code=404, detail="DataSource not found or access denied")

    dataset = Dataset(
        name=dataset_in.name,
        datasource_id=dataset_in.datasource_id,
        status="pending",
        owner_id=current_user.id  # è‡ªåŠ¨è®¾ç½®ä¸ºå½“å‰ç”¨æˆ·
    )
    db.add(dataset)
    db.commit()
    db.refresh(dataset)
    
    # Auto-generate collection_name based on ID
    dataset.collection_name = f"vec_ds_{dataset.id}"
    db.commit()
    db.refresh(dataset)
    
    return dataset

@router.get("/", response_model=List[DatasetResponse])
def list_datasets(
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    List datasets.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šæ™®é€šç”¨æˆ·åªèƒ½æŸ¥çœ‹è‡ªå·±çš„æ•°æ®é›†å’Œå…¬å…±èµ„æº
    """
    query = db.query(Dataset)
    query = apply_ownership_filter(query, Dataset, current_user)
    datasets = query.offset(skip).limit(limit).all()
    return datasets

@router.get("/{id}", response_model=DatasetResponse)
def get_dataset(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get a single dataset by ID.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šåªèƒ½æŸ¥çœ‹è‡ªå·±çš„æ•°æ®é›†å’Œå…¬å…±èµ„æº
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    return dataset

@router.get("/{id}/tables")
def get_dataset_tables(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get tables and their schemas for a dataset.
    æ”¯æŒ DuckDB å’Œä¼ ç»Ÿæ•°æ®æºã€‚
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸º DuckDB æ•°æ®é›†
        if dataset.duckdb_path:
            # ä» DuckDB è·å–è¡¨ä¿¡æ¯
            from app.core.logger import get_logger
            logger = get_logger(__name__)
            
            logger.info(f"Getting tables from DuckDB for dataset {id}, duckdb_path: {dataset.duckdb_path}")
            logger.info(f"Dataset schema_config: {dataset.schema_config}")
            
            # è·å–æ•°æ®é›†ä¸­çš„æ‰€æœ‰è¡¨å
            table_names = dataset.schema_config or []
            
            if not table_names:
                logger.warning(f"Dataset {id} has no tables in schema_config")
                return []
            
            tables_info = []
            for table_name in table_names:
                try:
                    logger.info(f"Getting schema for table: {table_name}")
                    schema = DuckDBService.get_table_schema(dataset.duckdb_path, table_name)
                    logger.info(f"Schema for {table_name}: {schema}")
                    columns = [
                        {
                            'name': col['name'],
                            'type': col['type'],
                            'nullable': col.get('nullable', True),
                            'default': None
                        }
                        for col in schema
                    ]
                    tables_info.append({
                        'name': table_name,
                        'columns': columns
                    })
                    logger.info(f"Successfully loaded table {table_name} with {len(columns)} columns")
                except Exception as e:
                    logger.error(f"Failed to get schema for table {table_name}: {e}", exc_info=True)
                    tables_info.append({
                        'name': table_name,
                        'columns': []
                    })
            
            logger.info(f"Returning {len(tables_info)} tables from DuckDB: {[t['name'] for t in tables_info]}")
            return tables_info
        else:
            # ä¼ ç»Ÿæ•°æ®æºï¼šä» datasource è·å–è¡¨ä¿¡æ¯
            if not dataset.datasource:
                raise HTTPException(status_code=400, detail="Dataset has no associated datasource")
            
            from app.services.db_inspector import DBInspector
            from sqlalchemy import inspect as sa_inspect
            
            table_names = dataset.schema_config or []
            if not table_names:
                return []
            
            engine = DBInspector.get_engine(dataset.datasource)
            inspector = sa_inspect(engine)
            
            tables_info = []
            for table_name in table_names:
                try:
                    columns = inspector.get_columns(table_name)
                    column_info = [
                        {
                            'name': col['name'],
                            'type': str(col['type']),
                            'nullable': col.get('nullable', True),
                            'default': str(col.get('default')) if col.get('default') is not None else None
                        }
                        for col in columns
                    ]
                    tables_info.append({
                        'name': table_name,
                        'columns': column_info
                    })
                except Exception as e:
                    logger.warning(f"Failed to get columns for table {table_name}: {e}")
                    tables_info.append({
                        'name': table_name,
                        'columns': []
                    })
            
            return tables_info
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get tables: {str(e)}")

@router.put("/{id}/tables", response_model=DatasetResponse)
def update_tables(
    id: int,
    config_in: DatasetUpdateTables,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update selected tables (schema_config) for a dataset.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šåªèƒ½ä¿®æ”¹è‡ªå·±çš„æ•°æ®é›†
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥ä¿®æ”¹
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot modify public resources")
        
    dataset.schema_config = config_in.schema_config
    db.commit()
    db.refresh(dataset)
    return dataset

@router.post("/{id}/train")
def train_dataset(
    id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Trigger training for a dataset.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šåªèƒ½è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥è®­ç»ƒ
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot train public resources")
        
    if not dataset.schema_config:
        raise HTTPException(status_code=400, detail="No tables selected for training")
        
    # Check if already training? 
    if dataset.status == "training":
        # Optional: Allow restart or block? User didn't specify.
        # We'll allow it but maybe warn? For now just proceed.
        pass

    dataset.status = "pending" # Set to pending before background task picks it up (or directly training)
    # Actually VannaManager sets it to 'training'.
    # But to give immediate feedback, maybe we can set it here?
    # VannaManager logic:
    # dataset.status = "training"
    # So we don't strictly need to set it here, but it's good UI feedback if we set it to 'pending' or 'queued'.
    # The model default is 'pending'.
    
    background_tasks.add_task(run_training_task, id, dataset.schema_config)
    
    return {"message": "è®­ç»ƒå·²å¼€å§‹"}


# Business Term Management Endpoints
@router.post("/{id}/terms", response_model=BusinessTermResponse)
def add_business_term(
    id: int,
    term_in: BusinessTermCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Add a business term to a dataset and train it in Vanna.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šéœ€è¦éªŒè¯ Dataset çš„æ‰€æœ‰æƒ
    """
    # Verify dataset exists and user has access
    ds_query = db.query(Dataset).filter(Dataset.id == id)
    ds_query = apply_ownership_filter(ds_query, Dataset, current_user)
    dataset = ds_query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥æ·»åŠ æœ¯è¯­
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot add terms to public resources")
    
    # Create business term in database
    business_term = BusinessTerm(
        dataset_id=id,
        term=term_in.term,
        definition=term_in.definition,
        owner_id=current_user.id  # è‡ªåŠ¨è®¾ç½®ä¸ºå½“å‰ç”¨æˆ·
    )
    db.add(business_term)
    db.commit()
    db.refresh(business_term)
    
    # Train the term in Vanna
    try:
        VannaTrainingService.train_term(
            dataset_id=id,
            term=term_in.term,
            definition=term_in.definition,
            db_session=db
        )
        # æ¸…ç†è¯¥æ•°æ®é›†çš„ç¼“å­˜ï¼Œé¿å…è¿”å›è¿‡æ—¶çš„SQL
        VannaManager.clear_cache(id)
    except Exception as e:
        # Rollback database if Vanna training fails
        db.delete(business_term)
        db.commit()
        raise HTTPException(status_code=500, detail=f"è®­ç»ƒæœ¯è¯­å¤±è´¥: {str(e)}")
    
    return business_term


@router.get("/{id}/terms", response_model=List[BusinessTermResponse])
def list_business_terms(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get all business terms for a dataset.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šéœ€è¦éªŒè¯ Dataset çš„è®¿é—®æƒ
    """
    # Verify dataset access
    ds_query = db.query(Dataset).filter(Dataset.id == id)
    ds_query = apply_ownership_filter(ds_query, Dataset, current_user)
    dataset = ds_query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # æŸ¥è¯¢æœ¯è¯­ï¼ˆä¹Ÿåº”ç”¨éš”ç¦»ï¼‰
    term_query = db.query(BusinessTerm).filter(BusinessTerm.dataset_id == id)
    term_query = apply_ownership_filter(term_query, BusinessTerm, current_user)
    terms = term_query.all()
    
    return terms


@router.delete("/terms/{term_id}")
def delete_business_term(
    term_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Delete a business term from database.
    Note: Vanna Legacy API does not provide a direct way to remove specific training data,
    so this only removes from database. The term will remain in the vector store.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šåªèƒ½åˆ é™¤è‡ªå·±çš„æœ¯è¯­
    """
    term_query = db.query(BusinessTerm).filter(BusinessTerm.id == term_id)
    term_query = apply_ownership_filter(term_query, BusinessTerm, current_user)
    term = term_query.first()
    
    if not term:
        raise HTTPException(status_code=404, detail="Business term not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥åˆ é™¤
    if term.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot delete public resources")
    
    db.delete(term)
    db.commit()
    
    return {"message": "æœ¯è¯­å·²åˆ é™¤ï¼ˆæ³¨ï¼šå‘é‡åº“ä¸­çš„è®­ç»ƒæ•°æ®ä»ä¿ç•™ï¼‰"}


# ===== QA Training Endpoints =====

@router.post("/{id}/training/qa")
def train_qa_pair(
    id: int,
    qa_data: TrainQARequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    è®­ç»ƒä¸€ä¸ª QA å¯¹ï¼ˆé—®é¢˜-SQL å¯¹ï¼‰
    è¿™å°†å¸®åŠ© AI å­¦ä¹ å¦‚ä½•å°†ç‰¹å®šé—®é¢˜è½¬åŒ–ä¸º SQL æŸ¥è¯¢
    
    Args:
        id: æ•°æ®é›† ID
        qa_data: åŒ…å« question å’Œ sql çš„è¯·æ±‚ä½“
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥æ·»åŠ è®­ç»ƒæ•°æ®
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot add training data to public resources")
    
    try:
        VannaTrainingService.train_qa(
            dataset_id=id,
            question=qa_data.question,
            sql=qa_data.sql,
            db_session=db
        )
        # æ¸…ç†è¯¥æ•°æ®é›†çš„ç¼“å­˜ï¼Œé¿å…è¿”å›è¿‡æ—¶çš„SQL
        VannaManager.clear_cache(id)
        return {
            "message": "QAå¯¹è®­ç»ƒæˆåŠŸ",
            "question": qa_data.question,
            "sql": qa_data.sql
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"QAå¯¹è®­ç»ƒå¤±è´¥: {str(e)}")


@router.post("/{id}/training/doc")
def train_documentation(
    id: int,
    doc_data: TrainDocRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    è®­ç»ƒä¸€ä¸ªæ–‡æ¡£ï¼ˆä¸šåŠ¡è§„åˆ™ã€æè¿°ç­‰ï¼‰
    è¿™å°†å¸®åŠ© AI ç†è§£ä¸šåŠ¡ä¸Šä¸‹æ–‡å’Œè§„åˆ™
    
    Args:
        id: æ•°æ®é›† ID
        doc_data: åŒ…å« content çš„è¯·æ±‚ä½“
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥æ·»åŠ è®­ç»ƒæ•°æ®
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot add training data to public resources")
    
    try:
        # è·å– Legacy Vanna å®ä¾‹å¹¶è®­ç»ƒæ–‡æ¡£
        vn = VannaInstanceManager.get_legacy_vanna(id)
        vn.train(documentation=doc_data.content)

        # æ¸…ç†è¯¥æ•°æ®é›†çš„ç¼“å­˜ï¼Œé¿å…è¿”å›è¿‡æ—¶çš„SQL
        VannaManager.clear_cache(id)

        return {
            "message": "æ–‡æ¡£è®­ç»ƒæˆåŠŸ",
            "content": doc_data.content[:100] + "..." if len(doc_data.content) > 100 else doc_data.content
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£è®­ç»ƒå¤±è´¥: {str(e)}")


# Modeling Endpoints
@router.post("/analyze", response_model=AnalyzeRelationshipsResponse)
def analyze_relationships(
    request: AnalyzeRelationshipsRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Analyze potential relationships between tables using AI.
    æ”¯æŒä¸¤ç§æ•°æ®æºï¼š
    1. ä¼ ç»Ÿæ•°æ®æºï¼ˆMySQL/PostgreSQLï¼‰- ä½¿ç”¨ VannaAnalystService
    2. DuckDB æ•°æ®æº - ä½¿ç”¨ RelationshipAnalyzer
    
    åˆ¤æ–­é€»è¾‘ï¼š
    - ä¼˜å…ˆæŸ¥æ‰¾åŒ…å«æŒ‡å®šè¡¨çš„ DuckDB æ•°æ®é›†ï¼ˆduckdb_path ä¸ä¸ºç©ºï¼‰
    - å¦‚æœæ²¡æœ‰ DuckDB æ•°æ®é›†ï¼Œå†ä½¿ç”¨ä¼ ç»Ÿæ•°æ®æºåˆ†æ
    """
    import logging
    logger = logging.getLogger(__name__)
    
    if not request.table_names or len(request.table_names) == 0:
        raise HTTPException(status_code=400, detail="At least one table name is required")
    
    # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆæŸ¥æ‰¾ DuckDB æ•°æ®é›†ï¼ˆå¤šæ–‡ä»¶ä¸Šä¼ åœºæ™¯ï¼‰
    # æŸ¥æ‰¾åŒ…å«è¿™äº›è¡¨çš„ DuckDB æ•°æ®é›†
    duckdb_datasets = db.query(Dataset).filter(
        Dataset.duckdb_path.isnot(None),
        Dataset.owner_id == current_user.id
    ).all()
    
    # æ‰¾åˆ°åŒ…å«æ‰€æœ‰æŒ‡å®šè¡¨çš„ DuckDB æ•°æ®é›†
    target_duckdb_dataset = None
    for dataset in duckdb_datasets:
        if dataset.schema_config and all(
            table in dataset.schema_config for table in request.table_names
        ):
            target_duckdb_dataset = dataset
            break
    
    # æ–¹æ¡ˆ1ï¼šå¦‚æœæ‰¾åˆ° DuckDB æ•°æ®é›†ï¼Œä½¿ç”¨ RelationshipAnalyzer
    if target_duckdb_dataset:
        try:
            logger.info(
                f"Using RelationshipAnalyzer for DuckDB dataset {target_duckdb_dataset.id}, "
                f"tables: {request.table_names}"
            )
            
            # ä½¿ç”¨ RelationshipAnalyzer åˆ†æ
            relationships = RelationshipAnalyzer.analyze_relationships(
                dataset_id=target_duckdb_dataset.id,
                db_path=target_duckdb_dataset.duckdb_path,
                table_names=request.table_names
            )
            
            # è½¬æ¢ä¸º API å“åº”æ ¼å¼
            edges = [
                EdgeResponse(
                    source=rel['source'],
                    target=rel['target'],
                    source_col=rel['source_col'],
                    target_col=rel['target_col'],
                    type=rel.get('type', 'left'),
                    confidence=f"{rel.get('confidence', 'medium')} ({rel.get('data_overlap', 0):.1f}% overlap)"
                )
                for rel in relationships
            ]
            
            # è·å–èŠ‚ç‚¹ä¿¡æ¯ï¼ˆè¡¨ç»“æ„ï¼‰
            nodes = []
            for table_name in request.table_names:
                schema = DuckDBService.get_table_schema(target_duckdb_dataset.duckdb_path, table_name)
                fields = [
                    FieldResponse(
                        name=col['name'],
                        type=col['type'],
                        nullable=col.get('nullable', True)
                    )
                    for col in schema
                ]
                nodes.append(NodeResponse(table_name=table_name, fields=fields))
            
            return AnalyzeRelationshipsResponse(edges=edges, nodes=nodes)
        
        except ValueError as e:
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            logger.error(f"DuckDB relationship analysis failed: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")
    
    # æ–¹æ¡ˆ2ï¼šæ²¡æœ‰ DuckDB æ•°æ®é›†ï¼Œä½¿ç”¨ä¼ ç»Ÿæ•°æ®æºåˆ†æ
    if request.datasource_id:
        # Verify DataSource access
        ds_query = db.query(DataSource).filter(DataSource.id == request.datasource_id)
        ds_query = apply_ownership_filter(ds_query, DataSource, current_user)
        datasource = ds_query.first()
        
        if not datasource:
            raise HTTPException(status_code=404, detail="DataSource not found or access denied")
        
        # Find dataset for this datasource
        dataset_query = db.query(Dataset).filter(Dataset.datasource_id == request.datasource_id)
        dataset_query = apply_ownership_filter(dataset_query, Dataset, current_user)
        dataset = dataset_query.first()
        
        if not dataset:
            raise HTTPException(
                status_code=400, 
                detail="No dataset found for this datasource. Please create a dataset first."
            )
        
        try:
            logger.info(
                f"Using RelationshipAnalyzer for traditional datasource, "
                f"dataset_id: {dataset.id}, datasource_id: {request.datasource_id}"
            )
            
            from app.services.db_inspector import DBInspector
            engine = DBInspector.get_engine(datasource)
            
            # ä½¿ç”¨ RelationshipAnalyzer åˆ†æï¼ˆå¤ç”¨ç›¸åŒçš„æ™ºèƒ½åˆ†æé€»è¾‘ï¼‰
            relationships = RelationshipAnalyzer.analyze_relationships(
                dataset_id=dataset.id,
                table_names=request.table_names,
                engine=engine
            )
            
            # è½¬æ¢ä¸º API å“åº”æ ¼å¼ (edges)
            edges = [
                EdgeResponse(
                    source=rel['source'],
                    target=rel['target'],
                    source_col=rel['source_col'],
                    target_col=rel['target_col'],
                    type=rel.get('type', 'left'),
                    confidence=f"{rel.get('confidence', 'medium')} ({rel.get('data_overlap', 0):.1f}% overlap)"
                )
                for rel in relationships
            ]
            
            # è·å–èŠ‚ç‚¹ä¿¡æ¯ï¼ˆè¡¨ç»“æ„ï¼‰
            from sqlalchemy import inspect as sa_inspect
            inspector = sa_inspect(engine)
            nodes = []
            
            for table_name in request.table_names:
                try:
                    columns = inspector.get_columns(table_name)
                    fields = [
                        FieldResponse(
                            name=col['name'],
                            type=str(col['type']),
                            nullable=col.get('nullable', True)
                        )
                        for col in columns
                    ]
                    nodes.append(NodeResponse(table_name=table_name, fields=fields))
                except Exception as e:
                    logger.warning(f"Failed to get columns for {table_name}: {e}")
                    nodes.append(NodeResponse(table_name=table_name, fields=[]))

            return AnalyzeRelationshipsResponse(edges=edges, nodes=nodes)
            
        except ValueError as e:
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            logger.error(f"Analysis failed: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")
    
    # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ•°æ®é›†
    raise HTTPException(
        status_code=404,
        detail="æœªæ‰¾åˆ°åŒ…å«æŒ‡å®šè¡¨çš„æ•°æ®é›†ï¼Œè¯·å…ˆä¸Šä¼ æ•°æ®æˆ–è¿æ¥æ•°æ®æº"
    )


def _deduplicate_sql_columns(sql: str) -> str:
    """
    å¤„ç† SQL ä¸­çš„é‡å¤åˆ—åé—®é¢˜ã€‚
    è§£æ SELECT å­å¥ï¼Œä¸ºé‡å¤çš„åˆ—åæ·»åŠ è¡¨åˆ«åå‰ç¼€ã€‚
    
    ä¾‹å¦‚ï¼š
    - è¾“å…¥: SELECT u.user_id, o.user_id, p.product_id FROM ...
    - è¾“å‡º: SELECT u.user_id AS u_user_id, o.user_id AS o_user_id, p.product_id FROM ...
    """
    import re
    import logging
    logger = logging.getLogger(__name__)
    
    # æå– SELECT å’Œ FROM ä¹‹é—´çš„åˆ—å®šä¹‰éƒ¨åˆ†
    select_match = re.match(r'^\s*SELECT\s+(.+?)\s+FROM\s+', sql, re.IGNORECASE | re.DOTALL)
    if not select_match:
        logger.warning("Cannot parse SELECT clause, returning original SQL")
        return sql
    
    select_clause = select_match.group(1)
    rest_of_sql = sql[select_match.end() - 5:]  # ä¿ç•™ FROM åŠä¹‹åçš„éƒ¨åˆ†
    
    # è§£æå„åˆ—ï¼ˆè€ƒè™‘é€—å·åˆ†éš”ï¼‰
    # ç®€å•åˆ†å‰²ï¼Œè€ƒè™‘å¯èƒ½çš„æ¢è¡Œ
    columns = [col.strip() for col in select_clause.split(',')]
    
    # ç»Ÿè®¡åˆ—åå‡ºç°æ¬¡æ•°
    column_names = {}  # {base_name: [(alias, full_column_str), ...]}
    parsed_columns = []  # [(alias, base_name, original_str), ...]
    
    for col in columns:
        if not col:
            continue
        # è§£æåˆ—åï¼šå¯èƒ½æ˜¯ alias.column æˆ– alias.column AS new_name
        # è·³è¿‡å·²æœ‰ AS åˆ«åçš„åˆ—
        if ' AS ' in col.upper():
            parsed_columns.append((None, None, col))
            continue
        
        # åŒ¹é… alias.column_name æ ¼å¼
        col_match = re.match(r'^([a-zA-Z_][a-zA-Z0-9_]*)\s*\.\s*([a-zA-Z_][a-zA-Z0-9_]*)$', col.strip())
        if col_match:
            table_alias = col_match.group(1)
            column_name = col_match.group(2)
            parsed_columns.append((table_alias, column_name, col))
            if column_name not in column_names:
                column_names[column_name] = []
            column_names[column_name].append(table_alias)
        else:
            # æ²¡æœ‰è¡¨åˆ«åæˆ–è€…å¤æ‚è¡¨è¾¾å¼ï¼Œä¿æŒåŸæ ·
            parsed_columns.append((None, None, col))
    
    # æ‰¾å‡ºé‡å¤çš„åˆ—å
    duplicate_columns = {name for name, aliases in column_names.items() if len(aliases) > 1}
    
    if not duplicate_columns:
        logger.info("No duplicate columns found")
        return sql
    
    logger.info(f"Found duplicate columns: {duplicate_columns}")
    
    # é‡å»º SELECT å­å¥ï¼Œä¸ºé‡å¤åˆ—æ·»åŠ åˆ«å
    new_columns = []
    seen_columns = set()  # è¿½è¸ªå·²å¤„ç†çš„åˆ—ï¼Œé˜²æ­¢å®Œå…¨ç›¸åŒçš„åˆ—é‡å¤å‡ºç°
    
    for table_alias, column_name, original in parsed_columns:
        if table_alias is None or column_name is None:
            # ä¿æŒåŸæ ·
            new_columns.append(original)
        elif column_name in duplicate_columns:
            # é‡å¤åˆ—ï¼Œæ·»åŠ åˆ«å (table_column)
            alias_name = f"{table_alias}_{column_name}"
            # å¦‚æœè¿™ä¸ªå®Œå…¨ç›¸åŒçš„åˆ—å·²ç»å‡ºç°è¿‡ï¼Œè·³è¿‡
            full_key = f"{table_alias}.{column_name}"
            if full_key in seen_columns:
                continue
            seen_columns.add(full_key)
            new_columns.append(f"{original} AS {alias_name}")
        else:
            # éé‡å¤åˆ—ï¼Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            full_key = f"{table_alias}.{column_name}"
            if full_key in seen_columns:
                continue
            seen_columns.add(full_key)
            new_columns.append(original)
    
    # é‡å»º SQL
    new_select_clause = ',\n  '.join(new_columns)
    new_sql = f"SELECT \n  {new_select_clause}\nFROM {rest_of_sql[5:]}"  # å»æ‰å‰é¢çš„ FROM
    
    logger.info(f"Deduplicated SQL: {new_sql[:300]}...")
    return new_sql


def _train_relationships_from_edges(dataset_id: int, edges: list, db_session: Session):
    """
    ä» VueFlow edges è§£æè¡¨å…³ç³»å¹¶è®­ç»ƒåˆ° Vannaã€‚

    Args:
        dataset_id: æ•°æ®é›† ID
        edges: VueFlow edges æ•°æ®åˆ—è¡¨
        db_session: æ•°æ®åº“ä¼šè¯

    VueFlow Edge ç»“æ„ç¤ºä¾‹ï¼š
    {
        "id": "edge-1",
        "source": "node-users",  # èŠ‚ç‚¹ ID
        "target": "node-orders",
        "sourceHandle": "id",    # å­—æ®µå
        "targetHandle": "user_id",
        "data": {
            "sourceTable": "users",    # è¡¨å
            "targetTable": "orders",
            "sourceField": "id",
            "targetField": "user_id"
        }
    }

    Returns:
        dict: {
            "success": bool,
            "trained_count": int,
            "skipped_count": int,
            "validation_errors": list[str]
        }
    """
    import logging
    from app.services.db_inspector import DBInspector

    logger = logging.getLogger(__name__)

    result = {
        "success": True,
        "trained_count": 0,
        "skipped_count": 0,
        "validation_errors": []
    }

    if not edges or len(edges) == 0:
        logger.info(f"No edges to train for dataset {dataset_id}")
        return result

    # è·å– dataset å’Œå…³è”çš„ datasource ç”¨äºéªŒè¯
    dataset = db_session.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        result["success"] = False
        result["validation_errors"].append(f"Dataset {dataset_id} not found")
        return result

    datasource = db_session.query(DataSource).filter(DataSource.id == dataset.datasource_id).first()
    if not datasource:
        result["success"] = False
        result["validation_errors"].append(f"DataSource not found for dataset {dataset_id}")
        return result

    # ç¼“å­˜å·²éªŒè¯çš„è¡¨å’Œåˆ—ä¿¡æ¯ï¼Œé¿å…é‡å¤æŸ¥è¯¢
    validated_tables = {}  # table_name -> set(column_names)

    def validate_table_column(table_name: str, column_name: str) -> tuple[bool, str]:
        """éªŒè¯è¡¨å’Œåˆ—æ˜¯å¦å­˜åœ¨ï¼Œè¿”å› (is_valid, error_message)"""
        if table_name not in validated_tables:
            try:
                validation = DBInspector.validate_table_and_columns(datasource, table_name, [])
                if not validation["table_exists"]:
                    return False, f"è¡¨ '{table_name}' åœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨"
                # è·å–æ‰€æœ‰åˆ—åå¹¶ç¼“å­˜
                columns = DBInspector.get_column_names(datasource, table_name)
                validated_tables[table_name] = set(columns)
            except Exception as e:
                return False, f"éªŒè¯è¡¨ '{table_name}' æ—¶å‡ºé”™: {str(e)}"

        if column_name not in validated_tables[table_name]:
            return False, f"åˆ— '{column_name}' åœ¨è¡¨ '{table_name}' ä¸­ä¸å­˜åœ¨"

        return True, ""

    relationships = []

    for edge in edges:
        try:
            # æ–¹æ³•1ï¼šä» data ä¸­è·å–è¡¨åå’Œå­—æ®µåï¼ˆä¼˜å…ˆï¼‰
            if 'data' in edge and edge['data']:
                data = edge['data']
                source_table = data.get('sourceTable')
                target_table = data.get('targetTable')
                source_field = data.get('sourceField')
                target_field = data.get('targetField')
            else:
                # æ–¹æ³•2ï¼šä» source/target å’Œ handle ä¸­è§£æ
                # å‡è®¾èŠ‚ç‚¹ ID æ ¼å¼ä¸º "node-{table_name}"
                source_node_id = edge.get('source', '')
                target_node_id = edge.get('target', '')

                # æå–è¡¨åï¼ˆç§»é™¤ "node-" å‰ç¼€ï¼‰
                source_table = source_node_id.replace('node-', '') if source_node_id.startswith('node-') else source_node_id
                target_table = target_node_id.replace('node-', '') if target_node_id.startswith('node-') else target_node_id

                # Handle å³ä¸ºå­—æ®µå
                source_field = edge.get('sourceHandle', '')
                target_field = edge.get('targetHandle', '')

            # éªŒè¯å¿…è¦å­—æ®µ
            if not all([source_table, target_table, source_field, target_field]):
                error_msg = f"Edge {edge.get('id', 'unknown')}: ç¼ºå°‘å¿…è¦çš„è¡¨åæˆ–å­—æ®µå"
                logger.warning(error_msg)
                result["validation_errors"].append(error_msg)
                result["skipped_count"] += 1
                continue

            # P0: éªŒè¯è¡¨å’Œå­—æ®µæ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­
            is_valid, error = validate_table_column(source_table, source_field)
            if not is_valid:
                error_msg = f"Edge {edge.get('id', 'unknown')}: {error}"
                logger.warning(error_msg)
                result["validation_errors"].append(error_msg)
                result["skipped_count"] += 1
                continue

            is_valid, error = validate_table_column(target_table, target_field)
            if not is_valid:
                error_msg = f"Edge {edge.get('id', 'unknown')}: {error}"
                logger.warning(error_msg)
                result["validation_errors"].append(error_msg)
                result["skipped_count"] += 1
                continue

            # æ¨æ–­å…³ç³»ç±»å‹ï¼ˆåŸºäºå­—æ®µå‘½åçº¦å®šï¼‰
            is_fk_pattern = target_field.endswith('_id') or target_field == 'id'
            cardinality = "Many-to-One" if is_fk_pattern else "One-to-One (inferred)"

            # ç”Ÿæˆå¢å¼ºç‰ˆå…³ç³»æè¿°ï¼ˆå•å‘ï¼ŒåŒ…å« SQL ç¤ºä¾‹å’Œä¸šåŠ¡å«ä¹‰ï¼‰
            enhanced_desc = f"""## Table Relationship: {source_table} â†’ {target_table}

**Join Condition**: `{source_table}`.`{source_field}` = `{target_table}`.`{target_field}`

**Relationship Type**: {cardinality}
- The `{source_field}` column in `{source_table}` references `{target_field}` in `{target_table}`
- When querying {source_table} data with related {target_table} information, use LEFT JOIN

**Recommended SQL Pattern**:
```sql
SELECT s.*, t.*
FROM {source_table} s
LEFT JOIN {target_table} t ON s.{source_field} = t.{target_field}
```

**Business Context**: Each record in `{source_table}` is associated with one record in `{target_table}` through the `{source_field}` field."""

            relationships.append(enhanced_desc)

            logger.debug(f"Validated relationship: {source_table}.{source_field} <-> {target_table}.{target_field}")

        except Exception as e:
            error_msg = f"Edge {edge.get('id', 'unknown')}: è§£æå¤±è´¥ - {str(e)}"
            logger.warning(error_msg)
            result["validation_errors"].append(error_msg)
            result["skipped_count"] += 1
            continue

    if len(relationships) > 0:
        logger.info(f"Training {len(relationships)} validated relationship descriptions for dataset {dataset_id}")
        VannaTrainingService.train_relationships(dataset_id, relationships, db_session)
        result["trained_count"] = len(relationships)
    else:
        logger.info(f"No valid relationships extracted from {len(edges)} edges")

    result["success"] = len(result["validation_errors"]) == 0
    return result


def _analyze_sql_performance(engine, sql: str, db_type: str) -> dict:
    """
    åˆ†æ SQL æ€§èƒ½ï¼Œè¿”å›è­¦å‘Šä¿¡æ¯ã€‚

    Args:
        engine: SQLAlchemy å¼•æ“
        sql: è¦åˆ†æçš„ SQL è¯­å¥
        db_type: æ•°æ®åº“ç±»å‹ ('mysql', 'postgresql')

    Returns:
        dict: {
            "warnings": list[str],  # è­¦å‘Šä¿¡æ¯åˆ—è¡¨
            "estimated_rows": int,  # é¢„ä¼°è¡Œæ•°
            "has_full_scan": bool   # æ˜¯å¦æœ‰å…¨è¡¨æ‰«æ
        }
    """
    import logging
    logger = logging.getLogger(__name__)

    result = {
        "warnings": [],
        "estimated_rows": 0,
        "has_full_scan": False
    }

    try:
        with engine.connect() as conn:
            if db_type == 'mysql':
                # MySQL EXPLAIN
                explain_result = conn.execute(text(f"EXPLAIN {sql}"))
                rows = explain_result.fetchall()

                for row in rows:
                    # MySQL EXPLAIN åˆ—: id, select_type, table, type, possible_keys, key, key_len, ref, rows, Extra
                    row_dict = row._asdict() if hasattr(row, '_asdict') else dict(zip(explain_result.keys(), row))

                    # æ£€æµ‹å…¨è¡¨æ‰«æ
                    scan_type = row_dict.get('type', '')
                    if scan_type == 'ALL':
                        result["has_full_scan"] = True
                        table_name = row_dict.get('table', 'unknown')
                        result["warnings"].append(f"è¡¨ {table_name} å°†æ‰§è¡Œå…¨è¡¨æ‰«æ")

                    # ç´¯è®¡é¢„ä¼°è¡Œæ•°
                    rows_count = row_dict.get('rows', 0)
                    if rows_count:
                        result["estimated_rows"] += int(rows_count)

                    # æ£€æµ‹ä¸´æ—¶è¡¨ä½¿ç”¨
                    extra = row_dict.get('Extra', '') or ''
                    if 'Using temporary' in extra:
                        result["warnings"].append("æŸ¥è¯¢å°†ä½¿ç”¨ä¸´æ—¶è¡¨")
                    if 'Using filesort' in extra:
                        result["warnings"].append("æŸ¥è¯¢å°†ä½¿ç”¨æ–‡ä»¶æ’åº")

            elif db_type == 'postgresql':
                # PostgreSQL EXPLAIN (ä¸ä½¿ç”¨ ANALYZE é¿å…å®é™…æ‰§è¡Œ)
                explain_result = conn.execute(text(f"EXPLAIN (FORMAT JSON) {sql}"))
                explain_json = explain_result.fetchone()[0]

                if explain_json and len(explain_json) > 0:
                    plan = explain_json[0].get('Plan', {})

                    # æå–é¢„ä¼°è¡Œæ•°
                    result["estimated_rows"] = int(plan.get('Plan Rows', 0))

                    # æ£€æµ‹å…¨è¡¨æ‰«æ
                    node_type = plan.get('Node Type', '')
                    if node_type == 'Seq Scan':
                        result["has_full_scan"] = True
                        relation = plan.get('Relation Name', 'unknown')
                        result["warnings"].append(f"è¡¨ {relation} å°†æ‰§è¡Œé¡ºåºæ‰«æ (Seq Scan)")

                    # é€’å½’æ£€æŸ¥å­è®¡åˆ’
                    def check_plans(node):
                        if isinstance(node, dict):
                            if node.get('Node Type') == 'Seq Scan':
                                relation = node.get('Relation Name', 'unknown')
                                if f"è¡¨ {relation} å°†æ‰§è¡Œé¡ºåºæ‰«æ" not in str(result["warnings"]):
                                    result["warnings"].append(f"è¡¨ {relation} å°†æ‰§è¡Œé¡ºåºæ‰«æ (Seq Scan)")
                                    result["has_full_scan"] = True
                            for child in node.get('Plans', []):
                                check_plans(child)

                    check_plans(plan)

        # æ·»åŠ è¡Œæ•°è­¦å‘Š
        if result["estimated_rows"] > 100000:
            result["warnings"].append(f"é¢„ä¼°ç»“æœè¡Œæ•°è¾ƒå¤§ ({result['estimated_rows']:,} è¡Œ)ï¼ŒæŸ¥è¯¢å¯èƒ½è¾ƒæ…¢")
        elif result["estimated_rows"] > 1000000:
            result["warnings"].append(f"é¢„ä¼°ç»“æœè¡Œæ•°éå¸¸å¤§ ({result['estimated_rows']:,} è¡Œ)ï¼Œå¼ºçƒˆå»ºè®®æ·»åŠ ç´¢å¼•")

    except Exception as e:
        logger.warning(f"SQL æ€§èƒ½åˆ†æå¤±è´¥: {e}")
        # åˆ†æå¤±è´¥ä¸é˜»æ­¢è§†å›¾åˆ›å»ºï¼Œåªè®°å½•è­¦å‘Š
        result["warnings"].append(f"æ€§èƒ½åˆ†æè·³è¿‡: {str(e)}")

    return result


@router.post("/create_view")
def create_view(
    request: CreateViewRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Create or replace a database view based on the provided SQL.
    æ”¯æŒä¼ ç»Ÿæ•°æ®æºå’ŒDuckDBæ•°æ®é›†ä¸¤ç§æ¨¡å¼ï¼š
    - ä¼ ç»Ÿæ¨¡å¼ï¼šéœ€è¦ datasource_id
    - DuckDBæ¨¡å¼ï¼šéœ€è¦ dataset_id
    """
    # #region agent log
    import json; open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').write(json.dumps({"location": "dataset.py:create_view:entry", "message": "create_view called", "data": {"datasource_id": request.datasource_id, "dataset_id": request.dataset_id, "view_name": request.view_name, "user_id": current_user.id, "is_superuser": current_user.is_superuser}, "timestamp": __import__('time').time() * 1000, "sessionId": "debug-session", "hypothesisId": "H1,H3"}) + '\n'); open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').close()
    # #endregion
    
    datasource = None
    dataset = None
    
    # æ¨¡å¼1: DuckDBæ•°æ®é›†æ¨¡å¼
    if request.dataset_id:
        # #region agent log
        import json; open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').write(json.dumps({"location": "dataset.py:create_view:duckdb_mode", "message": "using DuckDB mode", "data": {"dataset_id": request.dataset_id}, "timestamp": __import__('time').time() * 1000, "sessionId": "debug-session", "hypothesisId": "H1"}) + '\n'); open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').close()
        # #endregion
        
        # æŸ¥æ‰¾å¹¶éªŒè¯Datasetæƒé™
        dataset_query = db.query(Dataset).filter(Dataset.id == request.dataset_id)
        dataset_query = apply_ownership_filter(dataset_query, Dataset, current_user)
        dataset = dataset_query.first()
        
        if not dataset:
            raise HTTPException(status_code=404, detail="Dataset not found or access denied")
        
        # ã€ä¿®å¤ã€‘æ£€æŸ¥æ˜¯å¦ä¸ºDuckDBæ•°æ®é›†ï¼Œå¦‚æœä¸æ˜¯ä¸”æœ‰datasource_idï¼Œåˆ™é™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
        if not dataset.duckdb_path:
            if dataset.datasource_id:
                # é™çº§åˆ°ä¼ ç»Ÿæ•°æ®æºæ¨¡å¼
                request.datasource_id = dataset.datasource_id
                dataset = None  # æ¸…ç©ºdatasetï¼Œä½¿ç”¨datasourceæ¨¡å¼
            else:
                raise HTTPException(status_code=400, detail="æ­¤æ•°æ®é›†æ—¢ä¸æ˜¯DuckDBæ•°æ®é›†ï¼Œä¹Ÿæ²¡æœ‰å…³è”æ•°æ®æº")
    
    # æ¨¡å¼2: ä¼ ç»Ÿæ•°æ®æºæ¨¡å¼
    if request.datasource_id and not dataset:
        # #region agent log
        import json; open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').write(json.dumps({"location": "dataset.py:create_view:datasource_mode", "message": "using DataSource mode", "data": {"datasource_id": request.datasource_id}, "timestamp": __import__('time').time() * 1000, "sessionId": "debug-session", "hypothesisId": "H1"}) + '\n'); open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').close()
        # #endregion
        
        # æŸ¥æ‰¾å¹¶éªŒè¯DataSourceæƒé™
        ds_query = db.query(DataSource).filter(DataSource.id == request.datasource_id)
        ds_query = apply_ownership_filter(ds_query, DataSource, current_user)
        datasource = ds_query.first()
        
        if not datasource:
            raise HTTPException(status_code=404, detail="DataSource not found or access denied")
        
        # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥åˆ›å»ºè§†å›¾
        if datasource.owner_id is None and not current_user.is_superuser:
            raise HTTPException(status_code=403, detail="Cannot create views on public datasources")
    
    else:
        raise HTTPException(status_code=400, detail="å¿…é¡»æä¾› datasource_id æˆ– dataset_id")
    
    # Validate view_name (prevent SQL injection)
    import re
    if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', request.view_name):
        raise HTTPException(status_code=400, detail="Invalid view name. Use only alphanumeric and underscore.")
    
    # Validate SQL (å¢å¼ºå®‰å…¨æ£€æŸ¥)
    sql_upper = request.sql.strip().upper()
    
    # 1. æ£€æŸ¥å¿…é¡»ä»¥ SELECT å¼€å¤´
    if not sql_upper.startswith('SELECT'):
        raise HTTPException(status_code=400, detail="SQL must be a SELECT query")
    
    # 2. æ£€æŸ¥å±é™©å…³é”®å­—
    DANGEROUS_KEYWORDS = [
        'DROP', 'DELETE', 'UPDATE', 'INSERT', 'TRUNCATE', 
        'ALTER', 'CREATE TABLE', 'CREATE INDEX', 'EXEC', 
        'EXECUTE', 'GRANT', 'REVOKE'
    ]
    
    for keyword in DANGEROUS_KEYWORDS:
        # ä½¿ç”¨å•è¯è¾¹ç•Œæ£€æŸ¥ï¼Œé¿å…è¯¯åˆ¤ï¼ˆå¦‚ "SELECTED" ä¸åº”åŒ¹é… "SELECT"ï¼‰
        if re.search(r'\b' + keyword + r'\b', sql_upper):
            raise HTTPException(
                status_code=400, 
                detail=f"SQL ä¸å…è®¸åŒ…å«å±é™©è¯­å¥: {keyword}"
            )
    
    try:
        from app.services.db_inspector import DBInspector
        import logging
        logger = logging.getLogger(__name__)

        logger.info(f"Creating view: {request.view_name}")

        # è‡ªåŠ¨å¤„ç†é‡å¤åˆ—åé—®é¢˜
        processed_sql = _deduplicate_sql_columns(request.sql)
        logger.info(f"Processed SQL: {processed_sql[:200]}...")

        # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„æ‰§è¡Œå¼•æ“
        if dataset and dataset.duckdb_path:
            # DuckDBæ¨¡å¼
            logger.info(f"Using DuckDB at: {dataset.duckdb_path}")
            
            # #region agent log
            import json; open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').write(json.dumps({"location": "dataset.py:create_view:duckdb_execution", "message": "creating view in DuckDB", "data": {"duckdb_path": dataset.duckdb_path}, "timestamp": __import__('time').time() * 1000, "sessionId": "debug-session", "hypothesisId": "H1"}) + '\n'); open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').close()
            # #endregion
            
            import duckdb
            
            # DuckDB ä½¿ç”¨ CREATE OR REPLACE VIEW è¯­æ³•
            create_view_sql = f"CREATE OR REPLACE VIEW {request.view_name} AS {processed_sql}"
            
            logger.info(f"Executing DuckDB CREATE VIEW: {create_view_sql[:200]}...")
            
            # æ‰§è¡Œåˆ›å»ºè§†å›¾ï¼ˆç›´æ¥ä½¿ç”¨duckdbè¿æ¥ï¼‰
            conn = duckdb.connect(dataset.duckdb_path)
            try:
                conn.execute(create_view_sql)
                conn.commit()
                
                # #region agent log
                import json; open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').write(json.dumps({"location": "dataset.py:create_view:duckdb_success", "message": "DuckDB view created successfully", "data": {"view_name": request.view_name}, "timestamp": __import__('time').time() * 1000, "sessionId": "debug-session", "hypothesisId": "H1"}) + '\n'); open('/Users/pusonglin/PycharmProjects/universal-bi/.cursor/debug.log', 'a').close()
                # #endregion
            finally:
                conn.close()
            
            logger.info(f"DuckDB view {request.view_name} created successfully")
            
            # DuckDBæ¨¡å¼çš„å“åº”ï¼ˆæš‚ä¸åˆ†ææ€§èƒ½ï¼‰
            return {
                "message": f"è§†å›¾ {request.view_name} åˆ›å»ºæˆåŠŸï¼ˆDuckDBï¼‰",
                "view_name": request.view_name,
                "dataset_id": dataset.id
            }
        
        else:
            # ä¼ ç»Ÿæ•°æ®æºæ¨¡å¼
            logger.info(f"Using DataSource ID: {request.datasource_id}")
            
            engine = DBInspector.get_engine(datasource)

            # æ€§èƒ½é¢„æ£€ï¼šåˆ†æ SQL æ‰§è¡Œè®¡åˆ’
            perf_analysis = _analyze_sql_performance(engine, processed_sql, datasource.type)
            if perf_analysis["warnings"]:
                logger.warning(f"SQL æ€§èƒ½è­¦å‘Š: {perf_analysis['warnings']}")

            # Create or replace view
            # Note: Syntax varies by database type
            if datasource.type == 'postgresql':
                create_view_sql = f"CREATE OR REPLACE VIEW {request.view_name} AS {processed_sql}"
            elif datasource.type == 'mysql':
                # MySQL requires dropping the view first if it exists
                drop_view_sql = f"DROP VIEW IF EXISTS {request.view_name}"
                create_view_sql = f"CREATE VIEW {request.view_name} AS {processed_sql}"

                logger.info(f"Executing DROP VIEW: {drop_view_sql}")
                with engine.connect() as conn:
                    conn.execute(text(drop_view_sql))
                    conn.commit()
            else:
                # Default to CREATE OR REPLACE
                create_view_sql = f"CREATE OR REPLACE VIEW {request.view_name} AS {processed_sql}"
            
            logger.info(f"Executing CREATE VIEW: {create_view_sql[:200]}...")
            
            # Execute create view
            with engine.connect() as conn:
                conn.execute(text(create_view_sql))
                conn.commit()
            
            logger.info(f"View {request.view_name} created successfully")

            # æ„å»ºå“åº”ï¼ŒåŒ…å«æ€§èƒ½è­¦å‘Šä¿¡æ¯
            response = {
                "message": f"è§†å›¾ {request.view_name} åˆ›å»ºæˆåŠŸ",
                "view_name": request.view_name,
                "performance": {
                    "warnings": perf_analysis.get("warnings", []),
                    "estimated_rows": perf_analysis.get("estimated_rows", 0),
                    "has_full_scan": perf_analysis.get("has_full_scan", False)
                }
            }
            
            # å¦‚æœæœ‰è­¦å‘Šï¼Œåœ¨æ¶ˆæ¯ä¸­æç¤º
            if perf_analysis.get("warnings"):
                response["message"] += "ï¼ˆæœ‰æ€§èƒ½è­¦å‘Šï¼Œè¯·æŸ¥çœ‹è¯¦æƒ…ï¼‰"
            
            return response
        
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Failed to create view: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºè§†å›¾å¤±è´¥: {str(e)}")


# ===== Training Progress Management Endpoints =====

@router.get("/{id}/training/progress")
def get_training_progress(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–è®­ç»ƒè¿›åº¦
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # è·å–æœ€æ–°çš„æ—¥å¿—ä½œä¸º current_step
    latest_log = db.query(TrainingLog).filter(
        TrainingLog.dataset_id == id
    ).order_by(TrainingLog.created_at.desc()).first()
    
    current_step = latest_log.content if latest_log else "ç­‰å¾…å¼€å§‹..."
    
    return {
        "status": dataset.status,
        "process_rate": dataset.process_rate,
        "error_msg": dataset.error_msg,
        "current_step": current_step
    }


@router.get("/{id}/training/logs", response_model=List[TrainingLogResponse])
def get_training_logs(
    id: int,
    limit: int = 50,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–è®­ç»ƒæ—¥å¿—
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # æŸ¥è¯¢æ—¥å¿—
    logs = db.query(TrainingLog).filter(
        TrainingLog.dataset_id == id
    ).order_by(TrainingLog.created_at.desc()).limit(limit).all()
    
    return logs


@router.get("/{id}/training/data", response_model=TrainingDataResponse)
def get_training_data(
    id: int,
    page: int = 1,
    page_size: int = 20,
    type_filter: str = None,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–å·²è®­ç»ƒçš„æ•°æ®ï¼ˆQAå¯¹ã€DDLã€æ–‡æ¡£ï¼‰
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šéœ€è¦éªŒè¯ Dataset çš„è®¿é—®æƒ
    
    Args:
        id: æ•°æ®é›† ID
        page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
        page_size: æ¯é¡µæ•°é‡ï¼ˆé»˜è®¤20ï¼‰
        type_filter: ç±»å‹ç­›é€‰ï¼Œå¯é€‰å€¼: 'ddl', 'sql', 'documentation'
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    try:
        # è°ƒç”¨ VannaTrainingDataService è·å–è®­ç»ƒæ•°æ®
        result = VannaTrainingDataService.get_training_data(id, page, page_size, type_filter)
        return result
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}")


@router.post("/{id}/training/pause")
def pause_training(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    æš‚åœè®­ç»ƒ
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    if dataset.status != "training":
        raise HTTPException(status_code=400, detail="Dataset is not training")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥æš‚åœ
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot pause training for public resources")
    
    dataset.status = "paused"
    db.commit()
    
    return {"message": "è®­ç»ƒæš‚åœè¯·æ±‚å·²å‘é€"}


@router.delete("/{id}/training/data/{training_data_id}")
def remove_single_training_data(
    id: int,
    training_data_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    åˆ é™¤å•æ¡è®­ç»ƒæ•°æ®ï¼ˆDDLã€æ–‡æ¡£æˆ– QA å¯¹ï¼‰

    Args:
        id: æ•°æ®é›† ID
        training_data_id: è®­ç»ƒæ•°æ® ID
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()

    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")

    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥åˆ é™¤
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot delete training data from public resources")

    success = VannaTrainingDataService.remove_training_data(id, training_data_id)

    if success:
        return {"message": "è®­ç»ƒæ•°æ®å·²åˆ é™¤", "id": training_data_id}
    else:
        raise HTTPException(status_code=404, detail="è®­ç»ƒæ•°æ®ä¸å­˜åœ¨æˆ–æ ¼å¼ä¸æ­£ç¡®")


@router.delete("/{id}/training")
def delete_training_data(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    åˆ é™¤è®­ç»ƒæ•°æ®ï¼ˆæ¸…ç† Collectionï¼‰
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥åˆ é™¤
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot delete training data for public resources")
    
    # è°ƒç”¨ VannaInstanceManager åˆ é™¤ collection
    success = VannaInstanceManager.delete_collection(id)

    if success:
        return {"message": "è®­ç»ƒæ•°æ®å·²æ¸…ç†"}
    else:
        raise HTTPException(status_code=500, detail="æ¸…ç†è®­ç»ƒæ•°æ®å¤±è´¥")


@router.delete("/{id}")
def delete_dataset(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    åˆ é™¤æ•°æ®é›†ï¼ˆåŒ…å«çº§è”åˆ é™¤è®­ç»ƒæ•°æ®ã€ä¸šåŠ¡æœ¯è¯­ã€è®­ç»ƒæ—¥å¿—ç­‰ï¼‰
    """
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥åˆ é™¤
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot delete public resources")
    
    # 1. åˆ é™¤ Vanna Collection (è®­ç»ƒæ•°æ®)
    try:
        VannaInstanceManager.delete_collection(id)
    except Exception as e:
        # è®°å½•æ—¥å¿—ä½†ç»§ç»­åˆ é™¤
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(f"Failed to delete collection for dataset {id}: {e}")
    
    # 2. åˆ é™¤æ‰€æœ‰å…³è”çš„å¤–é”®è®°å½•ï¼ˆä¿®å¤å¤–é”®çº¦æŸé”™è¯¯ï¼‰
    from app.models.metadata import ChatMessage, ChatSession, DashboardCard, ComputedMetric
    
    # åˆ é™¤å…³è”çš„èŠå¤©æ¶ˆæ¯
    db.query(ChatMessage).filter(ChatMessage.dataset_id == id).delete()
    
    # åˆ é™¤å…³è”çš„èŠå¤©ä¼šè¯ï¼ˆå°† dataset_id è®¾ä¸º NULLï¼Œå› ä¸ºå®ƒæ˜¯å¯é€‰çš„ï¼‰
    db.query(ChatSession).filter(ChatSession.dataset_id == id).update(
        {"dataset_id": None}
    )
    
    # åˆ é™¤å…³è”çš„çœ‹æ¿å¡ç‰‡
    db.query(DashboardCard).filter(DashboardCard.dataset_id == id).delete()
    
    # åˆ é™¤å…³è”çš„è®¡ç®—æŒ‡æ ‡
    db.query(ComputedMetric).filter(ComputedMetric.dataset_id == id).delete()
    
    # 3. åˆ é™¤æ•°æ®åº“è®°å½•ï¼ˆçº§è”åˆ é™¤ä¼šè‡ªåŠ¨åˆ é™¤ business_terms å’Œ training_logsï¼‰
    db.delete(dataset)
    db.commit()
    
    return {"message": "æ•°æ®é›†å·²åˆ é™¤"}


@router.put("/{id}/modeling-config")
def update_modeling_config(
    id: int,
    config: dict,
    train_relationships: bool = False,  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦ç«‹å³è®­ç»ƒå…³ç³»
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    æ›´æ–°æ•°æ®é›†çš„å»ºæ¨¡é…ç½®ï¼ˆä¿å­˜ç”»å¸ƒæ•°æ®ï¼‰ã€‚
    å½“ train_relationships=True æ—¶ï¼Œä¼šè§£æ edges å¹¶è®­ç»ƒè¡¨å…³ç³»åˆ° Vannaã€‚
    
    Args:
        id: æ•°æ®é›† ID
        config: å»ºæ¨¡é…ç½®ï¼ˆåŒ…å« nodes å’Œ edgesï¼‰
        train_relationships: æ˜¯å¦ç«‹å³è®­ç»ƒå…³ç³»ï¼ˆé»˜è®¤ Falseï¼‰
        db: æ•°æ®åº“ä¼šè¯
        current_user: å½“å‰ç”¨æˆ·
    """
    import logging
    logger = logging.getLogger(__name__)
    
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # é¢å¤–æ£€æŸ¥ï¼šå…¬å…±èµ„æºåªæœ‰è¶…çº§ç®¡ç†å‘˜å¯ä»¥ä¿®æ”¹
    if dataset.owner_id is None and not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Cannot modify public resources")
    
    # æ£€æµ‹ edges æ˜¯å¦å‘ç”Ÿå˜åŒ–
    old_edges = dataset.modeling_config.get('edges', []) if dataset.modeling_config else []
    new_edges = config.get('edges', [])
    
    edges_changed = False
    if len(old_edges) != len(new_edges):
        edges_changed = True
    else:
        # æ¯”è¾ƒ edge IDs
        old_edge_ids = {edge.get('id') for edge in old_edges if edge.get('id')}
        new_edge_ids = {edge.get('id') for edge in new_edges if edge.get('id')}
        edges_changed = old_edge_ids != new_edge_ids
    
    logger.info(f"Updating modeling config for dataset {id}, edges_changed={edges_changed}, train_relationships={train_relationships}")
    
    # ä¿å­˜é…ç½®
    dataset.modeling_config = config
    db.commit()
    db.refresh(dataset)
    
    # å¦‚æœè¿çº¿å‘ç”Ÿå˜åŒ–ä¸”ç”¨æˆ·è¦æ±‚è®­ç»ƒ
    if train_relationships and new_edges and len(new_edges) > 0:
        try:
            logger.info(f"Training relationships from {len(new_edges)} edges for dataset {id}")
            train_result = _train_relationships_from_edges(id, new_edges, db)

            # å¦‚æœæœ‰å…³ç³»è¢«è®­ç»ƒï¼Œæ¸…ç†ç¼“å­˜
            if train_result["trained_count"] > 0:
                VannaManager.clear_cache(id)

            # æ ¹æ®éªŒè¯ç»“æœæ„å»ºå“åº”
            if train_result["success"]:
                return {
                    "message": f"å»ºæ¨¡é…ç½®å·²ä¿å­˜ï¼Œ{train_result['trained_count']} ä¸ªè¡¨å…³ç³»å·²è®­ç»ƒ",
                    "modeling_config": dataset.modeling_config,
                    "relationships_trained": True,
                    "trained_count": train_result["trained_count"],
                    "edges_count": len(new_edges)
                }
            elif train_result["trained_count"] > 0:
                # éƒ¨åˆ†æˆåŠŸ
                return {
                    "message": f"å»ºæ¨¡é…ç½®å·²ä¿å­˜ï¼Œ{train_result['trained_count']} ä¸ªè¡¨å…³ç³»å·²è®­ç»ƒï¼Œ{train_result['skipped_count']} ä¸ªè·³è¿‡",
                    "modeling_config": dataset.modeling_config,
                    "relationships_trained": True,
                    "trained_count": train_result["trained_count"],
                    "skipped_count": train_result["skipped_count"],
                    "validation_errors": train_result["validation_errors"],
                    "edges_count": len(new_edges)
                }
            else:
                # å…¨éƒ¨éªŒè¯å¤±è´¥
                return {
                    "message": "å»ºæ¨¡é…ç½®å·²ä¿å­˜ï¼Œä½†æ‰€æœ‰è¡¨å…³ç³»éªŒè¯å¤±è´¥",
                    "modeling_config": dataset.modeling_config,
                    "relationships_trained": False,
                    "skipped_count": train_result["skipped_count"],
                    "validation_errors": train_result["validation_errors"]
                }
        except Exception as e:
            logger.error(f"Failed to train relationships: {e}", exc_info=True)
            # è®­ç»ƒå¤±è´¥ä¸å½±å“ä¿å­˜é€»è¾‘
            return {
                "message": "å»ºæ¨¡é…ç½®å·²ä¿å­˜ï¼Œä½†è¡¨å…³ç³»è®­ç»ƒå¤±è´¥",
                "modeling_config": dataset.modeling_config,
                "relationships_trained": False,
                "error": str(e)
            }
    
    return {
        "message": "å»ºæ¨¡é…ç½®å·²ä¿å­˜",
        "modeling_config": dataset.modeling_config,
        "relationships_trained": False
    }


@router.post("/upload_quick_analysis", response_model=DatasetResponse)
async def upload_file_for_quick_analysis(
    file: UploadFile = File(...),
    name: Optional[str] = Form(None),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ä¸Šä¼  Excel/CSV æ–‡ä»¶å¹¶è‡ªåŠ¨åˆ›å»ºæ•°æ®é›†è¿›è¡Œå¿«é€Ÿåˆ†æ
    
    å¤„ç†æµç¨‹:
    1. è¯»å–æ–‡ä»¶ä¸º DataFrame
    2. æ¸…æ´—åˆ—åä»¥å…¼å®¹ SQL
    3. å°†æ•°æ®å†™å…¥ç¬¬ä¸€ä¸ªå¯ç”¨æ•°æ®æº
    4. åˆ›å»º Dataset è®°å½•
    5. è‡ªåŠ¨è®­ç»ƒ DDL
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶ (CSV/Excel)
        name: æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºæ–‡ä»¶åï¼‰
        db: æ•°æ®åº“ä¼šè¯
        current_user: å½“å‰ç”¨æˆ·
    
    Returns:
        DatasetResponse: æ–°åˆ›å»ºçš„æ•°æ®é›†ä¿¡æ¯
    """
    import logging
    from app.utils.file_handler import read_file_to_df, sanitize_column_names
    from app.services.db_inspector import DBInspector
    from app.services.vanna import VannaTrainingService
    
    logger = logging.getLogger(__name__)
    
    # 1. è¯»å–æ–‡ä»¶ä¸º DataFrame
    logger.info(f"Reading uploaded file: {file.filename}")
    df = read_file_to_df(file)
    
    # 2. æ¸…æ´—åˆ—å
    df = sanitize_column_names(df)
    logger.info(f"Sanitized columns: {list(df.columns)}")
    
    # 3. è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ•°æ®æºï¼ˆä¼˜å…ˆé€‰æ‹©ç”¨æˆ·è‡ªå·±çš„ï¼‰
    datasource_query = db.query(DataSource)
    datasource_query = apply_ownership_filter(datasource_query, DataSource, current_user)
    datasource = datasource_query.first()
    
    if not datasource:
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æ•°æ®æºï¼Œå°è¯•è·å–ç³»ç»Ÿé»˜è®¤æ•°æ®æºï¼ˆowner_id ä¸º Noneï¼‰
        datasource = db.query(DataSource).filter(DataSource.owner_id == None).first()
    
    if not datasource:
        raise HTTPException(
            status_code=400, 
            detail="æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼Œè¯·å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®åº“è¿æ¥"
        )
    
    # 4. ç”Ÿæˆå”¯ä¸€è¡¨å
    timestamp = int(time.time())
    table_name = f"upload_{current_user.id}_{timestamp}"
    logger.info(f"Generated table name: {table_name}")
    
    try:
        # 5. å°† DataFrame å†™å…¥æ•°æ®åº“
        engine = DBInspector.get_engine(datasource)
        df.to_sql(
            name=table_name,
            con=engine,
            if_exists='replace',
            index=False,
            method='multi'  # æ‰¹é‡æ’å…¥æé«˜æ€§èƒ½
        )
        logger.info(f"Successfully wrote {len(df)} rows to table {table_name}")
        
    except Exception as e:
        logger.error(f"Failed to write data to database: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æ•°æ®å†™å…¥æ•°æ®åº“å¤±è´¥: {str(e)}"
        )
    
    # 6. åˆ›å»º Dataset è®°å½•
    dataset_name = name or file.filename.rsplit('.', 1)[0]  # é»˜è®¤ä½¿ç”¨æ–‡ä»¶å
    
    dataset = Dataset(
        name=dataset_name,
        datasource_id=datasource.id,
        schema_config=[table_name],  # è‡ªåŠ¨é…ç½®è¡¨
        status="pending",
        owner_id=current_user.id
    )
    db.add(dataset)
    db.commit()
    db.refresh(dataset)
    
    # è‡ªåŠ¨ç”Ÿæˆ collection_name
    dataset.collection_name = f"vec_ds_{dataset.id}"
    db.commit()
    db.refresh(dataset)
    
    logger.info(f"Created dataset {dataset.id} with table {table_name}")
    
    # 7. è‡ªåŠ¨è®­ç»ƒ DDLï¼ˆç®€æ˜“è®­ç»ƒï¼‰
    try:
        logger.info(f"Starting quick training for dataset {dataset.id}")
        
        # è·å– DDL
        ddl = DBInspector.get_table_ddl(datasource, table_name)
        
        # ä½¿ç”¨ Legacy Vanna è®­ç»ƒ DDL
        from app.services.vanna.instance_manager import VannaInstanceManager
        vn = VannaInstanceManager.get_legacy_vanna(dataset.id)
        vn.train(ddl=ddl)
        
        # æ›´æ–°çŠ¶æ€
        dataset.status = "completed"
        dataset.process_rate = 100
        dataset.last_train_at = datetime.utcnow()
        db.commit()
        db.refresh(dataset)
        
        logger.info(f"Quick training completed for dataset {dataset.id}")
        
    except Exception as e:
        logger.error(f"Training failed for dataset {dataset.id}: {e}")
        dataset.status = "failed"
        dataset.error_msg = str(e)
        db.commit()
        db.refresh(dataset)
    
    return dataset


@router.get("/{id}/suggested_questions", response_model=SuggestedQuestions)
async def get_suggested_questions(
    id: int,
    limit: int = 5,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–æ•°æ®é›†çš„æ¨èé—®é¢˜ï¼ˆçŒœä½ æƒ³é—®ï¼‰
    
    è¯¥åŠŸèƒ½æ ¹æ®æ•°æ®é›†çš„è¡¨ç»“æ„å’Œå…³é”®å­—æ®µï¼Œåˆ©ç”¨ LLM ç”Ÿæˆç”¨æˆ·æœ€å¯èƒ½æ„Ÿå…´è¶£çš„ä¸šåŠ¡åˆ†æé—®é¢˜ã€‚
    ç»“æœä¼šç¼“å­˜ 24 å°æ—¶ï¼Œå‡å°‘ LLM è°ƒç”¨æˆæœ¬ã€‚
    
    Args:
        id: æ•°æ®é›† ID
        limit: è¿”å›é—®é¢˜æ•°é‡ï¼ˆé»˜è®¤ 5ï¼‰
        db: æ•°æ®åº“ä¼šè¯
        current_user: å½“å‰ç”¨æˆ·
    
    Returns:
        SuggestedQuestions: åŒ…å«é—®é¢˜åˆ—è¡¨çš„å“åº”
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    query = db.query(Dataset).filter(Dataset.id == id)
    query = apply_ownership_filter(query, Dataset, current_user)
    dataset = query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # ç¼“å­˜ Key
    cache_key = f"suggested_questions:{id}:{limit}"
    
    try:
        # æ£€æŸ¥ Redis ç¼“å­˜
        from app.core.redis import redis_service
        cached_questions = await redis_service.get(cache_key)
        
        if cached_questions:
            import logging
            logger = logging.getLogger(__name__)
            logger.info(f"Returning cached suggested questions for dataset {id}")
            return SuggestedQuestions(questions=cached_questions)
        
        # ç”Ÿæˆæ¨èé—®é¢˜
        questions = VannaManager.generate_suggested_questions(
            dataset_id=id,
            db_session=db,
            limit=limit
        )
        
        # å­˜å…¥ Redis ç¼“å­˜ï¼ˆ24 å°æ—¶è¿‡æœŸï¼‰
        cache_ttl = 86400  # 24 hours
        await redis_service.set(cache_key, questions, expire=cache_ttl)
        
        return SuggestedQuestions(questions=questions)
        
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Failed to generate suggested questions: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ¨èé—®é¢˜å¤±è´¥: {str(e)}")


# ==================== è®¡ç®—æŒ‡æ ‡ CRUD ====================

from pydantic import BaseModel

class ComputedMetricCreate(BaseModel):
    """Create computed metric request"""
    name: str
    formula: str
    description: Optional[str] = None

class ComputedMetricResponse(BaseModel):
    """Computed metric response"""
    id: int
    dataset_id: int
    name: str
    formula: str
    description: Optional[str] = None
    created_at: datetime
    
    class Config:
        from_attributes = True


@router.get("/{id}/metrics", response_model=List[ComputedMetricResponse])
def get_computed_metrics(
    id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get all computed metrics for a dataset.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šåªèƒ½æŸ¥çœ‹è‡ªå·±çš„æ•°æ®é›†çš„æŒ‡æ ‡
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    dataset_query = db.query(Dataset).filter(Dataset.id == id)
    dataset_query = apply_ownership_filter(dataset_query, Dataset, current_user)
    dataset = dataset_query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # è·å–æ‰€æœ‰è®¡ç®—æŒ‡æ ‡
    metrics = db.query(ComputedMetric).filter(ComputedMetric.dataset_id == id).all()
    return metrics


@router.post("/{id}/metrics", response_model=ComputedMetricResponse)
def create_computed_metric(
    id: int,
    metric_in: ComputedMetricCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Create a new computed metric for a dataset.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šéœ€è¦éªŒè¯ Dataset çš„æ‰€æœ‰æƒ
    """
    # éªŒè¯ dataset è®¿é—®æƒé™
    dataset_query = db.query(Dataset).filter(Dataset.id == id)
    dataset_query = apply_ownership_filter(dataset_query, Dataset, current_user)
    dataset = dataset_query.first()
    
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found or access denied")
    
    # åˆ›å»ºè®¡ç®—æŒ‡æ ‡
    metric = ComputedMetric(
        dataset_id=id,
        name=metric_in.name,
        formula=metric_in.formula,
        description=metric_in.description,
        owner_id=current_user.id
    )
    db.add(metric)
    db.commit()
    db.refresh(metric)
    
    return metric


@router.put("/metrics/{metric_id}", response_model=ComputedMetricResponse)
def update_computed_metric(
    metric_id: int,
    metric_in: ComputedMetricCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update a computed metric.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šåªèƒ½æ›´æ–°è‡ªå·±çš„æŒ‡æ ‡
    """
    # æŸ¥æ‰¾æŒ‡æ ‡
    metric = db.query(ComputedMetric).filter(ComputedMetric.id == metric_id).first()
    
    if not metric:
        raise HTTPException(status_code=404, detail="Metric not found")
    
    # éªŒè¯æƒé™ï¼šéœ€è¦éªŒè¯ dataset çš„è®¿é—®æƒé™
    dataset_query = db.query(Dataset).filter(Dataset.id == metric.dataset_id)
    dataset_query = apply_ownership_filter(dataset_query, Dataset, current_user)
    dataset = dataset_query.first()
    
    if not dataset:
        raise HTTPException(status_code=403, detail="Access denied to this metric")
    
    # æ›´æ–°æŒ‡æ ‡
    metric.name = metric_in.name
    metric.formula = metric_in.formula
    metric.description = metric_in.description
    metric.updated_at = datetime.utcnow()
    
    db.commit()
    db.refresh(metric)
    
    return metric


@router.delete("/metrics/{metric_id}")
def delete_computed_metric(
    metric_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Delete a computed metric.
    åº”ç”¨æ•°æ®éš”ç¦»ï¼šåªèƒ½åˆ é™¤è‡ªå·±çš„æŒ‡æ ‡
    """
    # æŸ¥æ‰¾æŒ‡æ ‡
    metric = db.query(ComputedMetric).filter(ComputedMetric.id == metric_id).first()
    
    if not metric:
        raise HTTPException(status_code=404, detail="Metric not found")
    
    # éªŒè¯æƒé™ï¼šéœ€è¦éªŒè¯ dataset çš„è®¿é—®æƒé™
    dataset_query = db.query(Dataset).filter(Dataset.id == metric.dataset_id)
    dataset_query = apply_ownership_filter(dataset_query, Dataset, current_user)
    dataset = dataset_query.first()
    
    if not dataset:
        raise HTTPException(status_code=403, detail="Access denied to this metric")
    
    # åˆ é™¤æŒ‡æ ‡
    db.delete(metric)
    db.commit()
    
    return {"message": f"æŒ‡æ ‡ {metric.name} å·²åˆ é™¤"}
